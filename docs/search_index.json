[["index.html", "Statistique appliquée aux sciences de l’environnement et du vivant Statistique appliquée aux sciences de l’environnement et du vivant.", " Statistique appliquée aux sciences de l’environnement et du vivant David M. Carminati 2025-09-01 Statistique appliquée aux sciences de l’environnement et du vivant. Ce livre R a pour objectif d’introduire les notions essentielles aux analyses statistiques appliquées aux sciences du vivant. Il a été co-construit après 3 années d’intervention, sur les UE biostatistiques, avec les étudiants de l’Université de Vannes (UBS). "],["introduction-au-langage-de-programmation-r.html", "Chapter 1 Introduction au langage de programmation R 1.1 Rstudio et IDE. 1.2 Les pré-requis 1.3 Les boucles 1.4 Les fonctions (version intermédiaire)", " Chapter 1 Introduction au langage de programmation R L’objectif de ce premier cours magistral est de reprendre les bases concernant R et d’être capable : De comprendre comment se structure le langage de programmation R. D’aller chercher les informations au sein des jeux de données. De visualiser les informations des jeux de données. Je vous joins ici une vidéo qui résume l’ensemble des fonctions nécessaires pour traiter des données écologiques/biologiques/environnementals. https://www.youtube.com/watch?v=dQe3Z7hRG1s 1.1 Rstudio et IDE. Le langage de programmation R est un langage spécifique adapté aux statistiques/mathématiques permettant de faire des calculs et visualiser des résultats à travers une fenêtre graphique. C’est le langage préferentiel des biologistes/physiologistes et cliniciens que ce soit dans les secteurs privés (start up, biotech.) ou plublique (recherche, université, collectivité). On peut l’utiliser sous 2 formes. La console R sans IDE (integrated development environment), qui ne comprend qu’une fenêtre (i.e. la console) où l’on peut écrire des lignes de commandes et une forme avec IDE. Nous travaillerons exclusivement avec la forme IDE. L’IDE le plus connue de R est RStudio et il comprend 4 fénêtres : un script. une fenêtre listant l’environnement (objet, matrice, fonction) une fenêtre graphique/aide. la console de commande. Bien évidemment, vous pouvez modifier ces fenètres et leur agencement. 1.2 Les pré-requis 1.2.1 Les objects Le second point clé dans la compréhension de R, ce sont les objets. La plus part des éléments de votre environnement seront des objets. Ils peuvent prendre la forme d’une suite de nombre, d’une phrase, d’une matrice, d’un tableau… Lorsque vous donnez délibérement une valeur, une forme à votre objet, il s’enregistre automatiquement dans votre environnement R, votre espace de travail actuel. Un objet possède une longueur et/ou une dimension particulière. De plus, on peut appliquer des opérations ou des fonctions à ces objets. Un objet possède un type de donnée (data.frame, numeric, character, factor, matrix, function, …). Vous pouvez savoir de quel type il s’agit en utilisant la fonction class() ou mode(). Vous pourrez donc appliquer des opérations à des objets de type numeric. Il est important de voir ces suites de nombres comme des matrices numériques. Dans ce contexte, les opérations mathématiques que vous allez réaliser se feront pour chacunes des entités de la matrice. \\[ a = \\left\\{\\begin{array}{lr} 1\\\\ 2\\\\ \\dots\\\\ 10 \\end{array}\\right\\} \\] Premier exemple (on applique un carré à la première suite de nombre) : a &lt;- 1:10 b &lt;- a^2 print(a) ## [1] 1 2 3 4 5 6 7 8 9 10 print(b) ## [1] 1 4 9 16 25 36 49 64 81 100 Donc le carré est appliqué en ligne pour chacun des nombres. Deuxième exemple (on additionne deux suites de nombre de longueur identique, ici la vecteur \\(a\\)) : a &lt;- 1:10 print(&quot;Première suite de nombre =&quot;);print(a) ## [1] &quot;Première suite de nombre =&quot; ## [1] 1 2 3 4 5 6 7 8 9 10 print(&quot;Addition de la suite avec elle même =&quot;);print(a+a) ## [1] &quot;Addition de la suite avec elle même =&quot; ## [1] 2 4 6 8 10 12 14 16 18 20 De même, le premier chiffre de chaque série est additioné avec le premier chiffre de la seconde série. 1.2.1.1 Les variables En statistique, on distingue deux types de variables : Variables quantitatives : elles prennent des valeurs numériques et permettent de mesurer une quantité (ex. âge, taille, poids). Variables qualitatives : elles représentent des catégories ou des qualités et ne sont pas numériques (ex. sexe, couleur des yeux, type de régime). Chaque variable qualitative peut prendre différentes modalités, c’est-à-dire les valeurs possibles qu’elle peut adopter. Par exemple, pour la variable Couleur des yeux, les modalités peuvent être bleu, vert, marron. La différence clé est donc que la variable définit la caractéristique étudiée, tandis que les modalités représentent ses différentes catégories possibles. 1.2.2 Les fonctions Elles permettent de réaliser une action (généralement un calcul) lorsque l’on y fait appel. Le langage possède des fonctions natives, c’est-à-dire des actions/calculs qui sont déjà créer et implémenter dans R. Pour faire appel à des fonctions, il faut écrire le nom de la fonction dans la console et définir les arguments de la fonction entre des parenthèses. Les arguments sont à définir par l’utilisateur (vous) et devront être séparés par des virgule. Prenons l’exemple de la fonction rnorm(n, mean, sd). Cette fonction permet de réaliser une simulation de n individus qui suivront une loi normale d’une moyenne (mean) et d’un écrat-type (sd) que vous devez définir. Ici je représente, graphiquement, la distribution normale de la taille (en cm) de 100 individus, ayant une moyenne de 177 cm et un écart type de 1,2. Posez vous la question à chaque fois : Comment reconnaitre une fonction ? Combien d’arguments possède t’elle ? Quelle est son action ? layout(matrix(c(1,2), nrow=1)) Taille &lt;- rnorm(100, mean = 177, sd = 1.2) hist(Taille) hist(Taille, col=&quot;firebrick&quot;, breaks=40, main=&quot;Densité de la population\\n en fontion de la taille (cm)&quot;) Dans R vous avez la possibilité de réaliser vos propres fonctions (description en fin de cours et objectif du premier TP). 1.2.2.1 Les fonctions indispensables dans le data mining. c() : concaténation de suite de chiffres/characters/factors. object[ , ] : permet de sélectionner une (ou plusieurs) ligne(s) et une (ou plusieurs) colonne(s) d’un objet qui peut être de classe data.frame, matrix ou tibble. which() : permet de rechercher les élements d’un vecteur qui valide une condition. grep() : similaire à which en moins restrictif. Cette fonction permet de chercher une chaîne de caractère qui se trouve dans un vecteur. Exercices : Tester ces fonctions dans le prochain TP. Ne pas hésiter à utiliser la fonction help(), pour avoir accès aux arguments des fonctions que vous souhaitez utiliser. 1.2.2.2 Data mining sur les objets de classes numériques Les objets, que nous avons créé jusqu’à présent, sont des suites de chiffres et chaque chiffre a une position précise dans la suite. Il nous est alors possible de rechercher un nombre spécifique en fonction de sa position dans la suite. Ici je créé une suite allant de 0.8 à 9.8. Pourquoi la suite ne va pas jusqu’à 10.5 ? Créer une séquence de chiffre personalisée avec la fonction seq(). suite &lt;- 0.8:10.5 print(suite) ## [1] 0.8 1.8 2.8 3.8 4.8 5.8 6.8 7.8 8.8 9.8 La ligne de code ci-contre montre comment accéder au 5ème chiffre dans la suite. suite[5] ## [1] 4.8 Nous pouvons aussi chercher les chiffres avec des conditions spécifiques. Ici, je vais aller chercher les chiffres de la suite compris entre 1 et 5. which(suite&gt;1 &amp; suite&lt;5) ## [1] 2 3 4 5 Par ailleurs, il est possible dans R de créer des tableaux (matrices) et de travailler sur ces mêmes tableaux (comme dans Excel). Dans cet exemple, nous considérerons une matrice (tableau) de 3 lignes et 4 colonnes. \\[ X_{i,j} = \\left\\{\\begin{array}{lr} 1, 2,5, 7\\\\ 2, 2 , 4, 9\\\\ 3, 5, 7, 10 \\end{array}\\right\\} \\] Pour créer ce tableau, nous utiliserons la function matrix(), qui possède 4 arguments. Nous appelons ce tableau \\(mat\\), qui est un objet. Le premier argument correspond aux chiffres qui seront implémentés dans le tableau. Le second argument correspond au nombre de lignes. Le troisième argument correspond au nombre de colonnes. Enfin, le 4ème argument correspond à la manière d’implémenter les chiffres, ici nous spécifions que les chiffres seront implémentés par ligne. mat &lt;- matrix(c(1, 2, 5, 7, 2, 2 , 4, 9, 3, 5, 7, 10), nrow = 3, ncol=4, byrow=T) colnames(mat) = c(&quot;anglais&quot;, &quot;français&quot;, &quot;espagnol&quot;, &quot;latin&quot;) rownames(mat) = c(&quot;eleve 1&quot;, &#39;eleve 2&#39;, &quot;eleve 3&quot;) print(mat) ## anglais français espagnol latin ## eleve 1 1 2 5 7 ## eleve 2 2 2 4 9 ## eleve 3 3 5 7 10 On peut utiliser la fonction dim() pour accéder aux dimensions du tableau. dim(mat) ## [1] 3 4 Le premier chiffre nous informe du nombre de lignes et le second du nombre de colonnes. mat est donc un objet correspondant aux notes de 3 étudiants en fonction des disciplines. On peut aller chercher des lignes/colonnes qui nous interessent en utilisant les fonctions indispensables de data mining, introduites précédemment. which(colnames(mat)==&quot;anglais&quot;) ## On souhaite avoir le numéro de la colonne qui s&#39;appelle anglais. ## [1] 1 grep(&quot;ais&quot;,colnames(mat)) ## On souhaite avoir l&#39;ensemble des colonnes qui contiennent le mot &quot;ais&quot;. ## [1] 1 2 On peut très bien aller chercher l’information de manière manuelle, en sélectionnant les lignes ou colonnes sur lesquelles on souhaite travailler. Il est possible de réaliser des opérations sur les matrices et de les combiner. mat[1:2 , 1:2] ## Sélection débutante ## anglais français ## eleve 1 1 2 ## eleve 2 2 2 mat[which(rownames(mat)==&quot;eleve 1&quot;|rownames(mat)==&quot;eleve 2&quot;) , grep(&quot;ais&quot;,colnames(mat))] ## Sélection avancée ## anglais français ## eleve 1 1 2 ## eleve 2 2 2 1.2.3 Les data-frames Dans notre cours nous utiliserons un type particulier de tableau, le data.frame, qui est proche des tableaux de classe matrice. Ils permettent de gérer les données de manière plus facile et rapide. On peut facilement transformer la matrice que nous avons créer en objet de type data.frame grâce à la fonction as.data.frame(). Je vous présenterai 2 avantages à travailler sur data.frames plutôt que sur des objets de classe matrix. df &lt;- as.data.frame(mat) # Première étape : transformer l&#39;objet matrix en objet data.frame class(df) # Vérifier la classe de l&#39;objet. ## [1] &quot;data.frame&quot; df$latin # Possibilité d&#39;aller chercher des colonnes de manière spécifique avec $. ## [1] 7 9 10 df$grec &lt;- c(3,8,2) # Possibilité d&#39;ajouter des colonnes avec ce même $, qui n&#39;existait pas avant. Il faut que le vecteur qu&#39;on ajoute soit de la même longueur. # Question : quelle est la dimension de notre nouveau tableau ? 1.3 Les boucles L’intérêt de R est de pouvoir réaliser des boucles, c’est-à-dire des itérations pour réaliser des calculs sur des tableaux de données importants. L’exemple le plus simple est de réaliser la moyenne des éleves à partir d’une boucle. Une boucle se réalise avec la fonction for() ou while(). La différence entre ces deux fonctions réside dans leur nature d’arrêt. Pour la fonction for(), nous allons assigner un objet, une séquence (numérique ou de character) à une lettre, et pour chaque élément de la liste/objet/séquence nous réaliserons l’action ou les actions qui seront inclus dans la boucle. Pour la fonction while(), l’argument sera une condition. Tant que cette condition ne sera pas remplie alors la boucle continuera son action. Pour l’exemple suivant, \\(i\\) prendra ne nom de chacun des élèves. l’itération n°1 consistera à créer la phrase suivante : “L’élève 1 possède une moyenne de …”. Ici mean(as.numeric(df[which(rownames(df)==i),]))) permet de calculer la moyenne de la ligne où la condition suivante which(rownames(df)==i) est respectée. Littérallement, cette condition signifie que nous recherchons le numéro de la ligne de \\(i\\) (ici \\(i\\) correspond à “eleve 1” lors de la première itération, puis “eleve 2” dans la seconde itération). Cette ligne de code nous permet donc de calculer l’ensemble des moyennes des élèves. for(i in rownames(df)){ print(paste(i,&quot;possède une moyenne de&quot;, mean(as.numeric(df[which(rownames(df)==i),])))) } ## [1] &quot;eleve 1 possède une moyenne de 3.6&quot; ## [1] &quot;eleve 2 possède une moyenne de 5&quot; ## [1] &quot;eleve 3 possède une moyenne de 5.4&quot; # Quel est le meilleur éleve ? Vous allez pouvoir utiliser la fonction apply ou rowMeans pour avoir les mêmes résultats. 1.3.0.1 Exercices Créer une matrice de 200 lignes et 10 colonnes. Simuler les données (200x10 = 2000 données) qui suivront une loi normale de moyenne 3 et d’écart type 1. Ajouter une colonne à ce tableau (200 observations d’une variable), la nouvelle variable suivra une loi normale de moyenne 2 et d’écart type 2. Calculer la moyenne en colonne (par variable) puis la moyenne en ligne (par individu/observation) en utilisant la fonction rowMeans() puis en utilisant la fonction apply(). 1.4 Les fonctions (version intermédiaire) Ce chapitre ne sera sans doute pas vu en cours mais vous êtes libres de le lire pour vos propres connaissances personnelles. Il pourra être approfondi lors du TP n°1. L’objectif de cette partie est de créer ses propres fonctions. Je vais vous l’expliquer en prenant un exemple assez simple puis travailler l’ensemble des notions du cours à partir de cette fonction. La première étape est de créer une fonction avec les différents arguments séparés par des virgules. Dans cet exemple, nous avons créé une fonction qui permet de calculer l’Indice de Masse Corporelle à partir de 2 arguments (la taille en mètre et le poids en kg). calcul_IMC &lt;- function(poids, taille){ IMC = poids/taille^2 return(IMC) } Dans la seconde partie, nous créons une boucle pour calculer un ensemble d’IMC possible soit Ta = P = BMI = NULL for(kg in seq(40,100, by=0.5)){ for(metres in seq(1.40,2.40, by=0.01)){ P &lt;- c(P, kg) Ta &lt;- c(Ta, metres) BMI &lt;- c(BMI , calcul_IMC(kg, metres)) } } df &lt;- data.frame(P, Ta, BMI) coupes &lt;- cut(df$BMI, c(5, 15, 18.5, 25, 30, 55)) colors = as.character(factor(coupes,c(levels(coupes)), c(&quot;firebrick&quot;, &quot;orange&quot;, &quot;darkgreen&quot;, &quot;orange&quot;, &quot;firebrick&quot;) )) plot(df[,1:2],col=t(colors), pch=15, cex=0.8, xlab= &quot;Poids (kg)&quot;, ylab=&quot;Taille (m)&quot;, main=&quot;Relation entre Poids et Taille&quot;, xlim=c(25, 100)) legend(&#39;topleft&#39;, legend =c(&#39;&lt;15 ou &gt;30&#39;,&#39;&lt;18.5 ou &gt;25&#39;,&#39;18.5&gt;IMC&lt;25&#39;), fill=c(&quot;firebrick&quot;, &quot;orange&quot;, &quot;darkgreen&quot;),title = &quot;IMC&quot;,cex=0.8, bty=&#39;n&#39;) "],["test-statistique-chi2.html", "Chapter 2 Test statistique (\\(\\chi^2\\)) 2.1 Observation/Expérience/causalité (quelques mots de vocabulaire) 2.2 Test statistique avec 2 variables qualitatives. 2.3 Révisions 2.4 Travaux Pratiques", " Chapter 2 Test statistique (\\(\\chi^2\\)) Pourquoi faire des tests statistiques ? Faire des tests statistiques permet de donner un cadre objectif à l’interprétation des résultats expérimentaux. En effet, lorsqu’on observe une différence entre deux groupes ou une corrélation entre deux variables, il peut être difficile de savoir si cette observation reflète un phénomène réel ou simplement le fruit du hasard. Le test statistique répond à cette question en quantifiant la probabilité d’obtenir un tel résultat si aucune relation véritable n’existait. Autrement dit, il permet d’évaluer si l’événement observé est rare — et donc digne d’attention car peu compatible avec l’hypothèse de hasard — ou au contraire commun, et alors probablement sans signification particulière. Ainsi, les tests statistiques offrent un outil essentiel pour distinguer les signaux réels des fluctuations aléatoires, et donc pour fonder les conclusions scientifiques sur une base rigoureuse. 2.1 Observation/Expérience/causalité (quelques mots de vocabulaire) 2.1.1 Échantillons/populations Une population comprend tous les individus ou objets d’intérêt. Les données sont collectées à partir d’un échantillon, qui est un sous-ensemble de la population. Exemple : pour estimer quel est le risque d’effet secondaire dans le traitement du cancer de la prostate par radio-thérapie, des chercheurs ont mené une étude sur une cohorte de plus de 300 patients. Pendant 5 ans après le traitement, ils ont surveillé la survenue d’effets secondaires. Ils ont trouvé que 35% de ces personnes ont souffert d’effets secondaires. Quelle est l’échantillon dans cette étude ? Qu’est-ce qu’une population raisonnable à laquelle nous pourrions généraliser les conclusions de l’étude ? Ceci est indispensable à garder à l’esprit. En effet, les études sont menées sur des échantillons QUI DOIVENT ÊTRE REPRÉSENTATIF DE LA POPULATION. Dans ce context, il est possible d’inférer les résultats. L’inférence statistique est le processus d’utilisation des données d’un échantillon pour obtenir des informations sur la population. Exemple : “On infère sa moyenne.” Attention : le choix de l’échantillon peut-être biaisé et les conclusions de l’étude ne pourraient être généralisées à l’ensemble de la population. Par rapport au graphique ci-contre, prenez le temps de vous informer sur les processus d’échantillonage possibles, leur point fort et leur point faible. Figure 1 - Schéma représentant deux processus d’échantillonage 2.1.2 Données/individu/variables En statistique, les données issues d’un échantillon, sont rangées de telle sorte que : chaque ligne correspond à un(e) individu/observation. chaque colonne correspond à une variable. Elles sont rangées dans un tableau que nous nommerons désormais une matrice \\(X_{ij}\\) avec \\(i\\) pour les individus/observations (lignes) et \\(j\\) pour les variables (colonnes). 2.1.3 Méthodologie d’analyse La première étape lorsque vous réalisez une analyse est de décrire les données. Quels types de données ? Quels types de variables ? Comment peut-on représenter le problème biologique de manière visuel ? Pour rappel (Chapitre 1), il existe principalement deux types de variables : Les variables quantitatives (ex : un taux de cholesterol). Les variables qualitatives (ex : des stades de cancers). La représentation visuelle et les statistiques associées seront totalement différentes. La visualisation dépendra du type de la variable à décrire et du problème associé. Pour chacune des problématiques que nous présenterons dans ce cours, il faudra déployer cette méthodologie : Définir la(es) variable(s) à visualiser. Définir un graphique permettant de visualiser le problème. Décrire numériquement les variables. Estimer des paramètres de l’échantillon ? Poser les hypothèses statistiques. Réaliser un test statistique pour valider ou refuser l’hypothèse nulle. Conclure en restant objectif(ve) par rapport aux résultats. 2.2 Test statistique avec 2 variables qualitatives. 2.2.1 Statistiques descriptives et visualisations Dans le code ci joint, nous importons des données que nous stockons dans une table de type data.frame, concernant des sportifs de Haut niveau (SHN) et leur type de microbiote intestinaux. Dans cette étude les auteurs souhaitaient savoir si les SHN présentaient des microbiotes intestinaux différents de ceux des sujets Non-athlètes (NA). Autrement dit la variable “SHN” présente si les individus (observations) sont des Sportifs de Haut Niveau (TRUE) ou non (FALSE, i.e. non athlètes). La variable “Type” prend deux valeurs soit 1 soit 2, qui définit le type de microbiote intestinal que possède l’individu. Brievement, les individus ayant un microbiote de type 1 possèdent des espèces bactériennes similaires et différents de ceux du type 2. Inversement pour les microbiote de type 2. Les données peuvent être téléchargées ici. Quelles sont les types des variables ? La première étape est de transformer la matrice en table de contingence (table à double entrée) où sont répertoriées des données de comptage entre deux variables qualitatives. df &lt;- read.table(&quot;data/SHN-microbiote.csv&quot;, sep=&quot;,&quot;, header=T, stringsAsFactors = T, encoding = &#39;latin1&#39;, row.names=1) knitr::kable(table(df)) 1 2 FALSE 3 18 TRUE 18 11 Questions Pouvons-nous utiliser ces données pour déterminer si le sport influence la composition du microbiote intestinal ? Attention au verbe utilisé dans la phrase ! Quelle est la population d’intérêt ? Cette question est l’une des plus importantes puisqu’elle conditionnera votre capacité à faire des conclusions à partir de votre analyse (inférence statistique). Il faut toujours se poser la question de comment a été echantillonné votre jeu de données et représente-t’il la population d’intérêt que vous souhaitez étudier. En statistique, on travaille souvent sur des échantillons. Dans ce cadre, on dira toujours qu’un paramètre (e.g. moyenne, variance, etc…) est estimé et on y ajoutera un chapeau. Ainsi, une probabilité estimée s’écrira \\(\\hat{p}_{j}\\) ou une moyenne s’écrira \\(\\hat{\\mu}_{j}\\) pour une variable \\(j\\). Quelle est la proportion de SHN dans la population (\\(\\hat{p}_{SHN}\\)) ? Proportion de SHN qui présente un microbiote de type 2 (\\(\\hat{p}_{SHN|M=2}\\)) ? Proportion de NA (Non Athlètes) qui possède un microbiote de type 2 (\\(\\hat{p}_{NA|M=2}\\)) ? Trouver et interpréter \\(\\hat{p}_{SHN}\\) − \\(\\hat{p}_{NA}\\), la différence de proportion entre SHN et NA. La proportion est une statistique qui aide à décrire la variable catégorielle pour ce groupe de jeunes hommes. Nous voyons que la proportion de SHN est égale à 58%. Une proportion (ou probabilité) se situera toujours entre 0 et 1 (ou bien 0 et 100%). Comment peut-on représenter cette probabilité ? Utiliser la fonction pie(). Vous avez toujours la possibilité d’aller de demander l’aide de R. SHN_NA &lt;- c(sum(df$SHN==TRUE),sum(df$SHN==FALSE)) ## (1) Faire la somme des SHN et NA, peu importe le type de microbiote intestinal et (2) l&#39;enregister dans un nouvel objet R. par(mar=c(0,0,0,0)) # Permet de conditionner les marges du plot. pie(SHN_NA, labels = c(&quot;SHN&quot;, &quot;NA&quot;), col=c(&quot;firebrick&quot;,&quot;cornflowerblue&quot;)) ## Utiliser la fonction pie. Figure 2.1: Figure 2 : Représentation en camembert des proportions de SHN vs NA au sein de l’échantillon étudié. Le problème sous jacent est donc le suivant : Est-ce que la proportion de SHN est la même dans le groupe des microbiote de type 1 que dans le groupe des microbiote de type 2 ? Cette question porte sur la relation entre les deux variables, et non l’influence. ATTENTION : Assurez-vous de lire attentivement les questions lorsque vous utilisez une table à double entrée. Les questions ”Quelle proportion de SHN possède un microbiote de type 1 ? ” Et ” Quelle est la proportion de microbiote de type 1 parmi les SHN» semblent similaires mais posent des questions différentes. Quel graphique pour repondre à la question du problème ? Que peut-on dire ? Peut-on conclure ? Causalité ? df.proportion.Sport &lt;- apply(table(df), 2, function(x) {x/sum(x)}) # je calcul les proportions en ligne. df.proportion.Microbiote &lt;- apply(table(df), 1, function(x) {x/sum(x)}) # je calcul les proportions en colonne. layout(matrix(c(1,2,3), nrow=1)) # permet de réaliser une fenêtre graphique à 3 cases barplot(as.matrix(df.proportion.Sport), col=c(&quot;firebrick&quot;,&quot;cornflowerblue&quot;), xlab=&quot;Type de Microbiote&quot;) # premier plot barplot(as.matrix(df.proportion.Microbiote), col=c(&quot;darkgreen&quot;,&quot;green&quot;), xlab=&quot;Sports&quot;) # second plot plot.new() # j&#39;ajoute un troisième plot vide pour pouvoir placer la légende. legend(&quot;center&quot;,c(&#39;SHN&#39;, &#39;NA&#39;, &quot;Type 1&quot;, &quot;Type 2&quot;), fill=c(&quot;firebrick&quot;,&quot;cornflowerblue&quot;,&quot;darkgreen&quot;,&quot;green&quot;), bty=&quot;n&quot;) Figure 2.2: Figure 3 : (1) Représentation des proprotions de SHN/NA (rouge/bleue) en fonction du type de microbiote (2) Représentation des types de microbiote (vert foncé/vert clair) en fonction du niveau de sport. Dans notre cas les deux graphiques ne sont pas différents de par la structure de notre table de contingence mais il pourrait l’être donc pensez à faire les deux graphiques/plot pour ne pas manquer les informations. N’OUBLIEZ JAMAIS QUE RELATION NE SIGNIFIE PAS CAUSALITÉ. 2.2.2 Explication des Tests Statistiques 2.2.2.1 Généralités Un test statistique (ou test d’hypothèses) utilise des données d’un échantillon pour décider entre deux hypothèses concernant une population en contrôlant le risque de se tromper. Dans notre exemple où nous avons 2 variables qualitatives, nous souhaitons répondre à la question suivante : La proportion de SHN est la même dans les individus présentant un microbiote de type 1 que dans le groupe des individus présentant un microbiote de type 2. Pour estimer la probabilité d’une dépendance entre deux variables qualitatives, vous pourrez utiliser le test du \\(\\chi^2\\). Son rôle est d’estimer et comparer la répartition des fréquences des modalités observées par rapport à une répartition théorique. Cette répartition théorique dépend de la question de recherche posée. La valeur de \\(\\chi^2\\) identifie la probabilité que la répartition observée diffère de la répartition théorique sous l’hypothèse nulle , en fonction du nombre de degrés de liberté (\\(dl, k\\)). Hypothèse nulle \\(H_0\\), en pratique, on commence par étudier ce qui se passe sous l’hypothèse correspondant au cas où il n’y pas de différence. On pose \\(H_0\\) qui se lit “Hypothèse nulle” où les deux variables sont indépendantes (ici, le fait d’être sportif est indépendant du type de microbiote OU BIEN le type de microbiote intestinal est indépendant du faite d’être sportif de haut niveau). Hypothèse alternative \\(H_1\\) , dans l’hypothèse alternative, on indique que les variables sont dépendantes. Hypothèse nulle (\\(H_0\\)) : Prétendre qu’il n’y a pas d’effet ou pas de dépendance. Hypothèse alternative (\\(H_1\\)) : Revendication pour laquelle nous recherchons des preuves significatives. L’hypothèse alternative est établie en observant les preuves (données) qui contredisent l’hypothèse nulle et soutiennent l’alternative hypothèse. 2.2.2.2 Erreurs liées à la prise de décision (p value) On fait une erreur quand on se trompe de conclusion. Il y a 2 façons de se tromper : Lorsque que l’on rejette \\(H_0\\) alors que \\(H_0\\) vraie : Erreur de Type 1 (Faux positifs) Lorsque que l’on ne refuse pas \\(H_0\\) alors que \\(H_0\\) fausse : Erreur de Type 2. (Faux négatifs) On sait estimer la probabilité de se tromper quand on rejette \\(H_0\\) (i.e. p.value). A retenir, dans un test statistique, vous calculez deux paramètres : la statistique du test et la p value. Plus la p value tend vers 0 et plus le phénomène est rare sous \\(H_0\\). Le degré de signification (ou p-value) nous donne un moyen formel de mesurer la “force” de la preuve qu’un échantillon fournit contre l’hypothèse nulle et à l’appui de l’hypothèse alternative. Autrement dit, la p value nous informe sur le risque que l’on prend à rejetter \\(H_O\\) à tort. Dans le cas du test du \\(\\chi^2\\), par exemple, plus le phénomène est rare sous \\(H_0\\) et plus la dépendance entre deux variables qualitatives est importante. 2.2.3 Test Statistique pour variables qualitatives (\\(\\chi^2\\)) Le test du \\(\\chi^2\\) va comparer les fréquences observées (O=observed) avec les fréquences attendues (E=expected) si H0 est correcte ou si nous choississons l’hypothèse alternative avec les taux d’erreurs associés. Si nous repartons de notre tableau de contingence, nous avons observons les répartitions ci contre. Sports // type 1 2 Total NA 3 18 21 SHN 18 11 29 Total 21 29 50 Question Quels sont les probabilités attendues (Expected) sous H0? Nous observons que notre population se compose à 58% de SHN donc si les variables était totalement indépendantes on s’attendrait à avoir 58% de SHN dans chacun des types de microbiote intestinaux. On peut prendre le problème dans l’autre sens, nous observons 42% de microbiote de type 1, on devrait retrouver ce microbiote chez 42% des SHN et chez 42% de NA. Réponse Etant donné que nous observons 21 microbiotes de type 1 (sur l’ensemble de notre échantillon) sur 50 individus, la probabilité \\(\\hat{p}_{T1}\\) est égale à 42% (ou 0.42), la théorie veut que 42% des SHN (0.42 x 29) et 42% des NA se trouvent dans la colonne 1 correspondant aux individus possédant un microbiote de type 1. Sports // Type 1 2 NA 21 x 0.42 = 8.82 21 x 0.58 = 12.18 SHN 29 x 0.42 = 12.18 29 x 0.58 = 16.82 Question Faites la même chose en faisant le raissonnement à partir du niveau de sports ? Le calcul de la statistique se définit donc par l’écart entre la répartition observée et la répartition théorique soit : \\[\\chi^2 = \\sum_{j,k} {\\frac{(O_{j,k}-E_{j,k})^2}{E_{j,k}}} \\] print(chisq.test(df$SHN, df$Enterotype)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: df$SHN and df$Enterotype ## X-squared = 9.5389, df = 1, p-value = 0.002012 Il faut retenir que le test du \\(\\chi^2\\) se base sur la différence entre ce que l’on observe et ce que l’on devrait observer sous \\(H_0\\). Conclusion Que faut-il conclure ? Est-ce que la pratique du sport impacte le type de microbiote intestinal ? ou bien est-ce le type de microbiote intestinal qui permet de devenir un athlète de Haut Niveau ? 2.3 Révisions 2.3.1 QCM 1. Le test du chi2 se réalise lorsque les deux variables sont : - [ ] Qualitatives - [ ] Quantitatives Afficher la réponse ✅ Qualitatives 2. H0 signifie que les variables sont dépendantes. - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux (H0 : les variables sont indépendantes) 3. Si la statistique de test du chi2 est grande alors sa p.value sera : - [ ] Faible - [ ] Élevée Afficher la réponse ✅ Faible 4. Une variable qualitative possède toujours 2 modalités : - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux (elle peut avoir un grand nombre de modalités) 5. Une probabilité est comprise entre -1 et 1 : - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux (elle possède une valeur entre 0 et 1) 6. Ce test statistique se base sur quel paramètre de position : - [ ] Moyenne - [ ] Médiane - [ ] Quantiles - [ ] Aucun Afficher la réponse ✅ Aucun 2.3.2 Questions ouvertes (définitions) Quel est le principe du calcul de la statistique de ce test ? Afficher la réponse Le principe est de comparer les probabilités observées (Observed) et les probabilités attendues (Expected) sous H0 (c’est-à-dire lorsque les variables sont totalement indépendantes). Donner la définition de H0 (Hypothèse Nulle) ? Afficher la réponse L’hypothèse nulle correspond à la situation où il n’existe aucune différence entre les conditions expérimentales ou bien que les groupes de variables étudiés soient indépendants. Le refus de H0 implique une prise de risque. Expliquez de manière vulgarisée. Afficher la réponse Le refus de H0 s’effectue après avoir pris connaissance de la p value. Plus la p value tend vers 0, plus le phénomène est rare sous H0. La p value mesure le risque que l’on prend à rejeter H0 à tort. Exemple : si p = 0.04, rejeter H0 signifie que l’on prend un risque de 4 % de se tromper. 2.3.3 Problème 2.3.3.1 Énoncé Vous devez évaluer un jeune médecin qui apprend à classer les individus SAINS ou MALADES. Pour ce faire, vous lui donnez des lames de microscope où se trouvent des biopsies de foie. Le médecin doit évaluer l’évolution de la stéatose hépatique (stockage de graisse). Vous récupérez ses verdicts sur 200 patients. Vous donnez les mêmes lames à un médecin expérimenté qui les classe à son tour. Votre objectif est de savoir si les deux médecins classent de la même manière les patients. La classification du médecin expérimenté sont en lignes et celle du jeune médecin en colonnes. 2.3.3.2 Tableau de classification SAINS (Jeune) MALADES (Jeune) SAINS (Exp.) 90 5 MALADES (Exp.) 10 95 Décrivez de manière littéraire la première case en haut à gauche. Afficher la réponse La case bleue nous donne le nombre de patients qui sont classés comme étant SAINS par les deux médecins. Quelle est la probabilité d’être classé MALADES par le médecin expérimenté lorsque le jeune médecin classe les patients comme étant MALADES ? Afficher la réponse 95 / (10+95) = 95 % Quelle est la probabilité d’être classé SAINS par le jeune médecin lorsque le médecin expérimenté les classe comme étant MALADES ? Que pouvez-vous dire de l’évaluation du jeune médecin ? Afficher la réponse 5 / (5+95) = 5 % → Le jeune médecin se trompe dans 5 % des cas en sous-classant des malades en sains. Définir H0 ? Puis définir H1 ? Afficher la réponse H0 : Les deux classifications effectuées par les médecins sont indépendantes (dissimilaires). H1 : Les deux classifications effectuées par les médecins sont dépendantes (proches). Calculer les probabilités de classification du jeune médecin sous \\(H_0\\) ? Afficher la réponse SAINS : 47,5 % MALADES : 52,5 % Calculer la statistique de test du \\(\\chi^2\\) pour la case en bas à droite ? Afficher la réponse Statistique de test = \\(\\frac{\\left(\\tfrac{95}{100} - \\tfrac{105}{200}\\right)^{2}}{\\tfrac{105}{200}}\\) La statistique du test définit une p value proche de 0.01. Conclure. Afficher la réponse On rejette H0 au seuil de 5 %, donc les deux médecins classent de manière significativement proche. 2.4 Travaux Pratiques 2.4.1 Pokemons Appliquer l’ensemble des analyses du chapitre sur un jeu de données concernant les Pokemons provenant de https://www.data.gouv.fr/fr/datasets/jeu-de-donnees-sur-les-pokemons/. Je vous mets à disposition une table à double entrée où sont présentés deux variables : (1) Le pokémon possède 1 ou 2 types (Roche, Eau, Feu, Plantes, etc…) et (2) le pokémon possède une évolution (Oui ou Non). Les données (pour créer ce tableau à double entrée) peuvent être téléchargées ici. Evolution // nb de type 1 type 2 types Oui 51 59 Non 64 41 Quelles sont les variables ? Poser \\(H_0\\) et \\(H_1\\) ? Réprésenter les résultats sous forme de graphique ? Définir l’erreur de type 1 ? Quelle conclusion ? 2.4.2 Travaux Pratiques Dans ce premier TP, votre objectif sera de déterminer le nombre d’heures de sports à réaliser par semaine associé avec un risque faible de développer une pathologie cardiovasculaire. L’objetif sous jacent de ce TP est de se familiriser avec la simulation des données par conséquent toutes les conclusions seront scienfitiquement infondées. Pour avoir accès à des résultats scientifiques fondés sur ce sujet, je vous invite à lire la review suivante : AHAJournal. Votre objectif est donc de déterminer un seuil concernant la pratique d’AP. 2.4.2.1 Partie simulation Définir une seed à 12 avec la fonction set.seed() set.seed(2) Définir un nombre de sujets à 200 que vous enregistrerez dans un objet n. n &lt;- 200 Simuler pour les n sujets un nombre d’heures de sport par semaine, grâce à une loi de poisson avec un lamdba de 4.2. Nous ajouterons un bruit gaussien. PA &lt;- rpois(n, 4.2)+rnorm(n, 0, 1) Quelle type de variable ? Visualiser l’hétérogénéité de la variable activité physique, i.e. PA. Définir un risque de développer une pathologie cardiovasculaire. La variable Risk comprendra 2 modalités (“Pas de risque” et “risque”). Vous devrez la créer à partir d’une loi binomiale où la probabilité de ne pas développer de risque (i.e. Pas de risque = 1) sera proportionnelle au niveau d’activité physique. Utiliser la fonction rbinom() pour introduire une probabilité de développé une pathologie cardiaque | nb heures de sport. Risk &lt;- NULL for(i in 1:n){ Risk &lt;- c(Risk, rbinom(1, 1, PA[i]/10+(0.1))) } ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced ## Warning in rbinom(1, 1, PA[i]/10 + (0.1)): NAs produced # Renommer la variable Risk (R) pour que le 1 soit égal à pas de risque. # Renommer la variable Risk (R) pour que le 0 soit égal à risque. Risk[Risk==1]=&quot;Pas_de_risque&quot; ; Risk[Risk==0]=&quot;risque&quot; Quelle type de variable ? 2.4.2.2 Partie test statistique Visualiser les données ensemble. boxplot(PA~Risk) Quelle est la probabilité d’avoir un risque de développer une pathologie cardiaque en fonction de la pratique du sport ? Réaliser une boucle pour nb_PA étant le nombre d’heure de sport allant de 1 à 8 heures par semaine pour déterminer la probabilité d’avoir un risque de développer une pathologie cardiaque. P_risk = NULL for (nb_PA in 1:8){ P_risk &lt;- c(P_risk, table(data.frame(Risk, PA&gt;=nb_PA))[2,2]/sum(table(data.frame(Risk, PA&gt;=nb_PA))[,2])) } Réaliser un graphique de la prababilité du risque en fonction de la pratique d’activité physique. plot(P_risk~c(1:8), ylim=c(0,1)) Quel seuil pourrions vous définir ? Utiliser le test du CHI2 pour vous aider à prendre une décision. Définir les hypothèses du test ? À quelle réponse serions nous capable de répondre ? –&gt; Est-ce que la proportion de sujet actif (seuil choisi) est la même dans le groupe “risque” que dans le groupe “pas de risque” ? Cette question porte sur la relation entre les deux variables. Réaliser une boucle pour nb_PA étant le nombre d’heure de sport allant de 1 à 8 heures par semaine pour déterminer la statistique de test et la p value du test du CHI2 p.value = statistique = NULL for (nb_PA in 1:8){ statistique &lt;- c(statistique, chisq.test(table(data.frame(Risk, PA&gt;=nb_PA)))$statistic) p.value &lt;- c(p.value, chisq.test(table(data.frame(Risk, PA&gt;=nb_PA)))$p.value) } ## Warning in chisq.test(table(data.frame(Risk, PA &gt;= nb_PA))): Chi-squared ## approximation may be incorrect ## Warning in chisq.test(table(data.frame(Risk, PA &gt;= nb_PA))): Chi-squared ## approximation may be incorrect Réaliser un graphique de la statistique du test en fonction de la pratique d’activité physique en colorant les points en fonction de la p.value. plot(statistique~c(1:8), col=p.value&lt;0.05) Dans quelle condition les variables sont les plus dépendantes entre elles. which.min(p.value) ## [1] 4 Visualiser la table de confusion à l’aide de la fonction barplot() avec le seuil choisi en ajoutant les résultats du test statistique (statistique du test + p value). Que peut-on conclure ? "],["tests-statistiques-t.-test-et-t.-wilcoxon.html", "Chapter 3 Tests statistiques (t. test et t. wilcoxon) 3.1 Statistiques descriptives et visualisations 3.2 Test Statistique pour variables quantitatives (Paramétriques / Non paramétriques) 3.3 Révisions", " Chapter 3 Tests statistiques (t. test et t. wilcoxon) Les tests abordés dans ce chapitre seront à mettre en place lorsqu’une des deux variables sera qualitative et l’autre quantitative. 3.1 Statistiques descriptives et visualisations L’ensemble des données de l’exemple ci-contre est issu d’un article paru dans Ecology (2023) intitulé High sensitivity of tropical forest birds to deforestation at lower altitudes par Mills SC et Collaborateurs (doi: 10.1002/ecy.3867). Il comprend des informations concernant le nombre d’espèces différentes observé sachant un paramètre de dépendence à la forêt. La survie des espèces d’oiseaux ont été classée comme étant moyennement dépendant de la forêt (“Medium.dependency”) ou bien fortement dépendant (“High.Dependency”). Enfin ce jeu de donnée présente la variable de l’atlitude moyenne associée à l’observation de ces espèces. Ces deux variables quantitatives sont associées à des sites géographiques de la cordillère des Andes (plus précisement en Colombie). Vous avez la possibilité de sauvegarder et d’uploader des datas en utilisant la forme de stockage .RDATA ou .rda. La fonction de stockage est save() et celle de chargement des données load(). Les données peuvent être téléchargées ici. load(file = &quot;data/Birds_dataset.rda&quot;) knitr::kable(head(birds)) Medium.dependency High.Dependency altitude Arcabuco Togui 142 100 2450.499 Asiria Belen 138 95 2893.601 Chingaza 127 85 3384.290 Chiquaque 142 100 2259.778 El Secreto 142 100 2320.463 El Taladro 142 100 2517.790 Questions Quelle est la population d’intérêt ? Comment pourrait-on répondre à la question suivante : Les sites échantillonnés sont-ils représentatifs de la Colombie en terme d’altitude ? Centre/position de la distribution Un graphique est utile pour nous aider à visualiser la forme d’une distribution. Nous pouvons aussi résumer les caractéristiques importantes d’une distribution numériquement. Les deux statistiques qui décrivent le centre ou l’emplacement d’une distribution pour une seule variable quantitative sont la moyenne et la médiane. La moyenne d’un échantillon de données \\(x_1, x_2,…, x_n\\) et d’une variable \\(j\\) (ici l’altitude) est définie par \\[\\hat{\\mu}_j = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\] Ici l’échantillonnage des oiseaux s’est réalisé en moyenne à 2297 mètres d’altitude soit \\(\\hat{\\mu}_{altitude} = 2297\\) Dispersion. La variance et l’écart-type donnent des informations sur la dispersion d’un échantillon. # Représenter la variable d&#39;altitude. hist(birds$altitude, col=&quot;lightblue&quot;, xlab=&quot;Altitude (mètres)&quot;,ylim=c(0,8), ylab=&quot;Comptage&quot;, main=&quot;Histogramme de l&#39;altitude des sites&quot;) # Représentation du paramètre de position. abline(v=mean(birds$altitude), col=&#39;red&#39;, lwd=2) abline(v=median(birds$altitude), col=&quot;blue&quot;, lwd=2) # Représentation du paramètre de dispersion. segments(x0 = mean(birds$altitude) - sd(birds$altitude), x1 = mean(birds$altitude) + sd(birds$altitude), y0 = 7.5, col=&quot;darkolivegreen&quot;, lwd=2) # On ajoute une légende pour comprendre notre graphique. legend(&quot;topleft&quot;, col=c(&quot;darkolivegreen&quot;,&quot;red&quot;,&quot;blue&quot;), lty=1, legend = c(&quot;sd&quot;, &quot;mean&quot;, &quot;median&quot;), lwd=2, bty=&quot;n&quot;) Figure 3.1: Figure 3 : Histogramme de l’Altitude. Ces statistiques descriptives primaires permettent de décrire les observations. Dans le cas on nous représentons la moyenne avec plus ou moins 1 écart-type, nous engloberons environ 66% des observations. Dans le cas où nous aurions choisi de représenter la moyenne avec plus ou moins 2 écart-types, on engloberait environ 95% des observations. Enfin dans le cas où nous choisirions de représenter la moyenne avec plus ou moins 3 écart-types, on engloberait environ 99% des observations. Voici un code pour savoir le pourcentage des observations si situant entre la Moyenne et + ou - 2 écart types. ecart_type &lt;- sd(birds$altitude) # définir l&#39;écart-type condition_moins_ecartT &lt;- which(birds$altitude &lt; (mean(birds$altitude) + (2*ecart_type))) # renvoie l&#39;ensemble des obsevrations pour lequel la condition suivant est respecté : obs INFÉRIEURE à la moyenne + ou - 2 SD. condition_plus_ecartT &lt;- which(birds$altitude &gt; (mean(birds$altitude) + (2*ecart_type))) # renvoie l&#39;ensemble des obsevrations pour lequel la condition suivant est respecté : obs SUPÉRIEURE à la moyenne + ou - 2 SD. length(condition_moins_ecartT | condition_plus_ecartT) / # renvoie le nombre d&#39;observation qui respecte les deux conditions précédentes. length(birds$altitude) # diviser par le nombre total d&#39;observations. ## [1] 0.9545455 Histogramme Une représentation alternative pour afficher une distribution de données quantitatives est un histogramme. En regroupant les données sur l’altitude en intervalles de 500 mètres (1000 à 1500 mètres, 1500 à 2000 mètres et ainsi de suite), nous obtenons le tableau des fréquences, présenté ci-contre. layout(matrix(c(1,2), nrow=1)) par(mar=c(5,5,2,2)) # Gérer les marges du plot. # PLOT N°1 boxplot(birds$Medium.dependency,birds$High.Dependency, names = c(&quot;Moyenne&quot;, &#39;Élevée&#39;), xlab=&#39;Niveau de dependance&#39;, ylab=&quot;Richesse d&#39;oiseaux (diversité)&quot;) # PLOT N°2 plot(birds$altitude, birds$Medium.dependency+birds$High.Dependency, xlab=&quot;Altitude (mètres)&quot;, ylab=&quot;Abondance d&#39;oiseaux\\n(Richesse)&quot;, main=&quot;&quot;) Figure 3.2: Figure 4 : Richesse en fonction de l’altitude. Ainsi, pour un échantillon \\(x_1, x_2,…, x_n\\) et une variable \\(j\\) (ici l’altitude), l’histogramme est défini par \\[\\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^{n} 1_{[x-h/2, x+h/2[}(x_{i})\\] où h est la largeur de bande et \\(1_A(x)\\) est la fonction indicatrice (ie elle vaut 1 si \\(x \\in A\\) et 0 sinon). La dependance à la présence de forêt influence t’elle la diversité des groupes d’oiseaux ? Répresentez le nombre d’espèces en fonction de l’altitude ? Que peut-on dire ? Y-a t’il un effet de l’altitude sur le nombre d’espèces. La dependance à la présence de forêt influence t’elle la diversité des groupes d’oiseaux observée avec l’altitude ? Autrement dit, la décroissance liée à l’altitude est-elle plus importante dans l’un des deux groupes ? par(mar=c(5,5,2,2)) # Gérer les marges du plot. # Il faut d&#39;abord normaliser les données pour pouvoir les comparer entre elles # Plusieurs normalisation existe, ici nous normaliserons les données par rapport à leur maximum. max.norm.M &lt;- birds$Medium.dependency/max(birds$Medium.dependency) max.norm.H &lt;- birds$High.Dependency/max(birds$High.Dependency) plot(birds$altitude, max.norm.M, xlab=&quot;Altitude (mètres)&quot;, ylab=&quot;Abondance d&#39;oiseaux\\n(Richesse)&quot;, main=&quot;&quot;, pch =15, col=&quot;darkgreen&quot;, ylim=range(c(max.norm.M,max.norm.H))) points(birds$altitude, max.norm.H, pch=16, col=&quot;gold&quot;) # Ajouter la légende. legend(&quot;bottomleft&quot;, legend = c(&quot;Moyenne&quot;, &quot;Élevée&quot;), title=&quot;Niveau de Dépendance&quot;, pch=c(15,16), col=c(&quot;darkgreen&quot;, &quot;gold&quot;), bty=&quot;n&quot;) Figure 3.3: Figure 5 : Richesse en fonction de l’altitude et de leur dépendence à la forêt. Quelles normalisations choisir pour comparer des données est un concept très important. Néanmoins il n’existe pas de recettes magiques. Tout dépend de l’analyse que vous souhaitez réaliser. 3.2 Test Statistique pour variables quantitatives (Paramétriques / Non paramétriques) 3.2.1 Généralités Un test statistique (ou test d’hypothèses) utilise des données d’un échantillon pour décider entre deux hypothèses concernant une population en contrôlant le risque de se tromper. Deux hypothèses : (1) L’altitude des sites/forêt d’echantillonnage sont similaires aux sites/forêts de la Colombie ou bien (2) l’altitude des sites/forêt d’echantillonnage sont différents aux sites/forêts de la Colombie (auxquels cas les conclusions de l’étude ne pourrait être généralisées sur l’ensemble de la colombie). Hypothèse nulle \\(H_0\\), l’hypothèse correspondant au cas où il n’y pas de différence. On dit que l’effet de la variable qualitative (Sites de l’étude / Sites de la Colombie) est nulle et on note : \\[H_0 : \\mu_E = \\hat{\\mu}_P\\] \\(\\mu_E\\) et \\(\\hat{\\mu}_C\\) sont respectivement l’altitude des sites échantillonés (dans l’étude) et l’altitude de l’ensemble des forêts/sites de la Colombie. Hypothèse alternative \\(H_1\\), dans l’hypothèse alternative, on indique que la variable qualitative (Sites de l’étude / Sites de la Colombie) est dépendante à une altitude d’échantillonnage. \\[H_1 : \\mu_E \\ne \\hat{\\mu}_P\\] Le problème consiste donc à décider entre les deux hypothèses : \\(H_0 : \\hat{\\mu}_M = \\hat{\\mu}_H\\) contre \\(H_1 : \\hat{\\mu}_M \\ne \\hat{\\mu}_H\\). 3.2.2 Prérequis pour le choix du test Les tests statistiques sont nombreux et vous devrez faire un choix par rapport aux natures des données sur lesquelles vous voulez tester des hypothèses. Les deux propriétés à vérifier avant de se lancer dans la réalisation d’un test sont (1) la normalité et (2) l’égalité des variances. Pour la suite de l’exemple, notre objectif est de comparer les altitudes entre les sites de l’étude (variable \\(X\\)) et ceux de la Colombie (variable \\(Y\\)). 1) Normalités Vous avez deux possibilités pour vérifier que votre échantillon est normal (i.e. qu’il suit une loi normale). Pour rappel, une loi normale est une distribution de probabilité qui dépend de 2 paramètres : une moyenne (qui décrit la position) et la variance (qui décrit la variabilité autour de la moyenne). Généralement, dans la nature, un grand nombre de processus sont gaussiens (taille, poids, bio-marqueurs, QI, etc…). La vérification du test de normalité permet de savoir si vous réaliserez un test statitistique basé sur des paramètres (i.e. position et dispersion) ou bien sur autres choses que des paramètres estimée \\(\\hat{\\theta}\\) de l’échantillon. La normalité doit être validée de deux manières : visuellement et à l’aide d’un test statitistique. Rq : si la taille de l’échantillon est grande (typiquement supérieure à 30), on peut supposer que la normalité est respectée. Attention certaines données sont très spécifiques et ne vérifie pas ce constat (i.e., données de comptage issues d’écosystème environnementaux). layout(matrix(c(1:4), nrow=1, byrow = T)) Etude &lt;- birds$altitude Colombie &lt;- c(rnorm(250, 2000, 700)) hist(Etude, main=&quot;Fonction de densité (Etude, X)&quot;, xlab=&quot;altitude&quot;) qqnorm(Etude) qqline(Etude) hist(Colombie, main=&quot;Fonction de densité (Colombie, Y)&quot;, xlab=&quot;altitude&quot;) qqnorm(Colombie) qqline(Colombie) Figure 3.4: Figure 6 : Lois normales. Quelles sont les hypothèses du test de normalité (i.e. définir \\(H_0\\), \\(H_1\\)) ? Est-ce que les 2 variables vous semblent normales ? En pratigue, on peut utiliser un test pour vérifier si les variables sont normales ou non. Dans le cadre de ce cours, nous utiliserons le test de Shapiro-Wilk. shapiro.test(Etude) ## ## Shapiro-Wilk normality test ## ## data: Etude ## W = 0.97695, p-value = 0.8623 shapiro.test(Colombie) ## ## Shapiro-Wilk normality test ## ## data: Colombie ## W = 0.99491, p-value = 0.573 Pour quelle(s) variable(s) accepte t’on \\(H_O\\) ? Pour quelle(s) variable(s) rejetons-nous \\(H_0\\) ? 2) Analyse des variances Dans un deuxième temps, il nous est nécessaire de vérifier si les échantillons possèdent une variance similaire. De la même manière, définissez quelles sont les hypothèses du test de la variance (i.e. définir \\(H_0\\), \\(H_1\\)) entre \\(E\\) et \\(P\\) ? De la même manière que pour la normalité, vous pouvez visualiser les données pour voir rendre compte de leur variance ? Vous pouvez aussi estimer leur paramètre de dispersion, si les valeurs sont proches il y a des chances pour que l’on accepte \\(H_0\\) (i.e. pas de différence de variance entre les deux variables). boxplot(Etude, Colombie, names = c(&quot;Etude (E)&quot;, &quot;Colombie (P)&quot;), ylab=&quot;Altitude&quot;, xlab=&quot;Echantillon versus Population&quot;, col=c(&quot;firebrick&quot;, &quot;cornflowerblue&quot;)) legend(&quot;topleft&quot;, legend = c(round(sd(Etude), 0),round(sd(Colombie), 0)), fill=c(&quot;firebrick&quot;, &quot;cornflowerblue&quot;), bty=&quot;n&quot;, cex=0.8, title = &quot;écart-types&quot;) Figure 3.5: Figure 7 : Étude des variances (paramètre de dispersion). Le test de Fisher-Snedecor est un test paramétrique qui permet de comparer les variances de deux échantillons indépendants. Pour plus de 2 échantillons, on utilise le test de Bartlett. var.test(Etude, Colombie) ## ## F test to compare two variances ## ## data: Etude and Colombie ## F = 0.86002, num df = 21, denom df = 249, p-value = 0.7136 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.4926687 1.7866287 ## sample estimates: ## ratio of variances ## 0.8600242 Nous pouvons faire les hypothèses selon lesquels la variable Altitude suit une loi normale dans les deux groupes (étude et Population) et avec des variances égales. 3.2.3 Test paramétrique (test de Student) On s’intéresse naturellement à l’écart entre les moyennes et on veut savoir si cet écart est loin ou non de 0. Et la variabilité de l’échantillon, mesurée par l’écart-type a bien sûr un impact sur la façon dont on considère le “loin de 0”. C’est pour cette raison que l’on doit en tenir compte. Comme pour l’ensemble des tests statistiques, la première étape est de calculer une statistique (ici \\(T\\), la statistique de test T de Student) et de savoir si la probabilité liée à la statitique est rare ou commune sous H0. Suivant la rareté de l’évenement, on pourra prendre une décision sur l’hypothèse à choisir. 1) Test de Student (variances égales) Si les variances sont égales entre les variables \\(X\\) et \\(Y\\) et qu’elles suivent des lois normales alors nous utiliserons un test de Student avec l’option variance égale. Nous allons d’abord le calculer à la main pour comprendre comment il fonctionne, voici la formule de la statistique \\(T\\) : \\[ T = \\frac{\\hat{\\mu}_{X}-\\hat{\\mu}_{Y}}{\\sqrt{\\frac{S^2_P}{n_X}+\\frac{S^2_P}{n_Y}}}\\] où \\(S^2_P\\) est l’estimation combinée de la variance (pooled variance) et se calcule comme suit : \\[S^2_P = \\frac{(n_X−1)S_X^2+(n_Y−1)S_Y^2}{n_X+n_Y-2} \\] où S correspond à la variance de \\(X\\) ou de \\(Y\\). Calculer la statistique \\(T\\) pour la variable Altitude dans les deux groupes (Sites de l’étude et Sites de la Colombie) ? n_x &lt;- length(Etude) # on enregistre le nombre d&#39;observations dans X n_y &lt;- length(Colombie) # on enregistre le nombre d&#39;observations dans Y ## On calcul les variances var_x &lt;- var(Etude) # on enregistre la variance de X var_y &lt;- var(Colombie) # on enregistre la variance de Y ## On calcul les moyennes mu_x &lt;- mean(Etude) mu_y &lt;- mean(Colombie) ## On calcul la variance poolée S2_P &lt;- ((n_x-1)*var_x + (n_y-1)*var_y)/(n_x+n_y-2) ## On peut finalement calculer la Statistique T S_T = (mu_x-mu_y) / sqrt(S2_P/(n_x-1) + S2_P/(n_y-1)) print(S_T) ## [1] 1.774692 tte statistique suit une loi de Student à \\(n\\) degrés de liberté qui nous informera sur la rareté de la différence sous \\(H_0\\) (sa probabilité d’observation selon une loi de student ou loi de Gauss si n est grand). Cette probabilité sera égal à sa p value. loi_de_student &lt;- rt(500, 270) hist(loi_de_student, main=&quot;Densité d&#39;une loi de Student&quot;, xlab=&quot;&quot;) # On construit une loi de student pour replacer notre statistique de T et calculer sa probabilité (i.e. sa rareté). abline(v=S_T) text(x=S_T+0.25,y=50 , &quot;Statistique T\\nde notre test&quot;, adj=0, xpd=&quot;NA&quot;) Pour rappel une probabilité est comprise entre 0 et 1. Il faut calculer l’aire des histogrammes qui sont supérieures à notre statistique de T que nous avons représentée par une ligne noire. length(which(loi_de_student&gt;S_T))/500 ## [1] 0.024 ## On peut aussi calculer la probabilité d&#39;observer la statistique T calculée directement avec la fonction de probabilité. 1-pt(S_T ,270) ## [1] 0.0385377 La probabilité d’observer cette différence de moyenne est-elle rare ? Quelle est la p value ? Quelle conclusion pouvez vous faire à partir de ces résultats ? Quelle est la probabilité d’erreur associée à votre prise de décision ? Plus rapidement, vous pouvez calculer la statistique T et sa p value associée avec la fonction t.test() sur R en ajoutant bien l’argument var.equal = T. t.test(Etude, Colombie, var.equal = T) ## ## Two Sample t-test ## ## data: Etude and Colombie ## t = 1.8134, df = 270, p-value = 0.07088 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -25.37686 617.65760 ## sample estimates: ## mean of x mean of y ## 2296.509 2000.368 Pourquoi observons-nous une différence dans la p.value du test R et de celui que nous avons calculé à la mains ? 2) Test de Student (variances inégales) Dans le cas où les variances ne sont pas égales, vous pouvez utilisez la même fonction dans R en utilisant l’argument var.egal = F. Attention, les variables doivent toujours être des variables normales. Quelle est la conclusion de l’étude des oiseaux par rapport à la qualité d’échantillonnage de l’étude des oiseaux ? Pour rappel, \\(H_0\\) l’altitude des sites d’échantillonage est représentatif des sites/forêts en Colombie. 3) Test de Student (Apparié) Dans le cas, on les observations/individus serait les mêmes dans les deux groupes. Par exemple, un même groupe de patient que l’on suit au cours du temps, il faudrait ajouter l’argument paired = T dans la fonction de t.test(X, Y, paired = T) 3.2.4 Test non paramétrique (test de Wilcoxon-Mann-Whitney) Si la variable d’intérêt ne suit pas une loi normale, i.e. qu’on ne peut se baser sur la moyenne et l’écart type pour calculer une statistique de test, il faut alors changer d’approche et calculer une statistique de test non paramétrique. Nous en verrons une pendant l’UE BIO1540 et une seconde pendant l’UE MTH1640. Dans cette UE, nous allons voir le Test de Wilcoxon. Il permet de comparer permet de comparer les positions de 2 échantillons \\(X = \\{x_1,··· ,x_n\\}\\) et \\(Y = \\{y_1,··· ,y_m\\}\\). \\(H_0 : \\hat{\\theta}_x=\\hat{\\theta}_y\\) contre \\(H_1 : \\hat{\\theta}_x \\neq \\hat{\\theta}_y\\)y où \\(\\hat{\\theta}\\) est par exemple l’estimation de la médiane. Il ne requiert pas d’hypothèse sur la distribution dont sont issues les données. En pratique on regroupe les 2 échantillons en un seul \\(z = \\{X,Y\\}\\) et on attribue un rang à chacune des observations de \\(z: z_{(1)} ≤ z_{(2)} ≤ ··· ≤ z_{(n+m)}\\) La statistique de test de Wilcoxon est \\[S_w = \\frac{n_x(n_x+1)}{2}-R_x\\] où \\(R_X\\) est la somme des rangs qui viennent du groupe \\(X\\). Globalement, la statistique de ce test évalue l’éloignement de la distribution des rangs d’une variable par rapport à une distribution des rangs alternés parfaitement entre les \\(X\\) et les \\(Y\\). Prenons comme exemple un jeu de données simulées de la diversité d’oiseaux dans les forêts Alpestres françaises. Nous souhaitons savoir quelles sont les forêts qui possèdent une diversité d’oiseaux plus importante ? Quels sont les hypothèses ? Visualiser les données. Calculer la statistique \\(S_w\\). Colombian_birds &lt;- (birds$Medium.dependency+birds$High.Dependency)+rnorm(22,0, 1) French_birds &lt;- c(rnorm(10, 200, 50),rnorm(10, 250, 10)) df &lt;- data.frame(Pays = c(rep(&quot;Colombie&quot;, length(Colombian_birds)),rep(&quot;France&quot;, length(French_birds))) , diversity = c(Colombian_birds, French_birds)) R_x &lt;- sum(rank(df$diversity)[which(df$Pays==&quot;France&quot;)]) # On récupère la somme des rangs associées au oiseaux de la France. paste(&#39;la statistique du test de wilcoxon est de : w = &#39;,22*20+((20*(20+1))/2)-R_x) ## [1] &quot;la statistique du test de wilcoxon est de : w = 201&quot; De la même manière que pour les autres tests statistiques, la statistique calculée nous permet de savoir si l’évènement est rare sous \\(H_0\\) auquel cas sa probabilité (p value) d’être observée sera faible. On peut vérifier la statistique de test avec la fonction wilcox.test() sur R. wilcox.test(diversity~Pays, data=df) ## ## Wilcoxon rank sum exact test ## ## data: diversity by Pays ## W = 201, p-value = 0.6447 ## alternative hypothesis: true location shift is not equal to 0 Quelle décision prendrez-vous ? Test de Wilcoxon (Apparié) Dans le cas, on les observations/individus serait les mêmes dans les deux groupes. Par exemple, un même groupe de patient que l’on suit au cours du temps, il faudrait ajouter l’argument paired = T dans la fonction de wilcox.test(X, Y, paired = T). A retenir : les tests statistiques présentés nous donne une probabilité que les différences dans nos données sont rares ou communes. La prise de décision dépend de vous et des erreurs que vous êtes prêts à faire. 3.3 Révisions 3.3.1 Rappel La statistique du T.TEST : \\[ T = \\frac{\\hat{\\mu}_{X}-\\hat{\\mu}_{Y}}{\\sqrt{\\frac{S^2_P}{n_X}+\\frac{S^2_P}{n_Y}}} \\] où \\(S^2_P\\) \\[ S^2_P = \\frac{(n_X−1)S_X^2+(n_Y−1)S_Y^2}{n_X+n_Y-2} \\] La statistique de test du Wilcoxon : dépend du nombre d’individus dans chaque groupe et de la somme des rangs pour l’un des groupes. \\[ S_w = \\frac{n_1 \\,(n_1 + 1)}{2} - R_1 \\] 3.3.2 QCM 1. Le T test (ou test de Student) se réalise lorsque les deux variables sont quantitatives. - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux 2. Le test de Wilcoxon est un test qui se base sur un paramètre de dispersion. - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux 3. Le T Test se base uniquement sur un paramètre de position. - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux 4. L’hypothèse nulle du Shapiro-Wilk est : la variable ne suit pas une loi normale. - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux (H0 : la variable suit une loi normale) 3.3.3 Questions ouvertes (définitions) Donnez la définition d’un test paramétrique. Afficher la réponse Un test paramétrique se base sur des paramètres estimés (exemple : moyenne, variance). Donnez la définition d’un test non-paramétrique. Afficher la réponse Un test non paramétrique ne se base pas sur des paramètres estimés. Il peut se baser sur des rangs, des signes ou des distributions. Sur quels éléments le test du Wilcoxon se base-t-il ? Afficher la réponse Le test de Wilcoxon se base sur des rangs. Sa statistique de test compare la somme des rangs théoriques sous \\(H_0\\) avec la somme des rangs observée. Sur quels éléments le test de T se base-t-il ? Afficher la réponse Le test de T se base sur les paramètres de position (moyenne) et de dispersion (variance). La statistique de test : \\[ T = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p} \\] où \\(s_p\\) est la variance poolée. Quels sont les tests préalables à réaliser avant de choisir si l’on applique un test paramétrique ou non paramétrique ? Afficher la réponse Vérification de la normalité des variables intra-groupe. Vérification de l’égalité des variances entre les deux groupes. Donner l’hypothèse nulle lorsque vous testez l’égalité des variances. Afficher la réponse H0 : Les deux groupes possède une variable \\(x\\) avec une variance identique. 3.3.4 Exercices 3.3.4.1 Exercice 1 Le boxplot exprime une variable quantitative en fonction d’une variable qualitative. Le groupe 1 et le groupe 2 sont deux groupes distincts. set.seed(1) boxplot(rnorm(10), rnorm(10, 0.5)) Questions : Dans cet exemple nous allons supposer que les variables sont normales et que les variances sont égales. Quel test devez-vous appliquer ? Définir H0. La moyenne du groupe 1 est de 0 et la moyenne du groupe 2 est de 0.6. La racine carrée de la somme des variances poolées divisée par le nombre d’individu dans chaque groupe est de 0.3. Calculer la statistique de test. La p-value est de 0.04. Conclure. Afficher la réponse Test : T.test (Student) H0 : Les deux groupes possèdent une moyenne identique. Statistique : \\[ T = \\frac{0 - 0.6}{0.3333} = -1.8 \\] Conclusion : p = 0.04 → on rejette H0 au seuil de 5 %. Les groupes ont des moyennes significativement différentes, avec un risque d’erreur de 4 %. 3.3.4.2 Exercice 2 On représente les valeurs individuelles sur l’axe des ordonnées, avec deux groupes colorés (1 point = 1 individu). La variable d’un des groupes n’est pas normale, alors que les sont variances sont proches. set.seed(1) df = data.frame(Group = rep(c(2,1), each = 5), value = c(rnorm(5), rnorm(5, 01.1))) plot(df$Group, df$value, xlab=&quot;&quot;, ylab=&quot;&quot;) Questions : Quel test appliquer ? Définir H1. Calculer la statistique de test à partir de la somme des rangs. La p-value est de 0.095. Conclure. Afficher la réponse Test : Wilcoxon H1 : Les deux groupes possèdent une distribution des rangs différente. Calcul : \\[S_w = \\frac{n_1 \\,(n_1 + 1)}{2} - R_1\\] \\[R_1 = (1+2+3+5+8) = 19\\] Soit vous compter sur le graphique à quelles positions sont les différents points, soit vous pouvez aussi vérifier avec la ligne de code suivante : rank(df$value)[which(df$Group==1)] ## [1] 4 7 10 9 6 paste(&quot;la somme des rangs pour le groupe 1 est de&quot;, sum(rank(df$value)[which(df$Group==1)])) ## [1] &quot;la somme des rangs pour le groupe 1 est de 36&quot; \\[ S_w = \\frac{5 \\,(5 + 1)}{2} - 19 \\] \\[ S_w = 15 - 19 = -4 \\] On peut aussi vérifier avec la ligne de code. On voit bien que la statistique de test wilcox.test(df$value~df$Group) ## ## Wilcoxon rank sum exact test ## ## data: df$value by df$Group ## W = 21, p-value = 0.09524 ## alternative hypothesis: true location shift is not equal to 0 Dans R, la statistique de test est la valeur absolue du test donc POSITIVE, mais il réalise le même calcul que nous. R trouve -4 comme nous mais propose sa valeur absolue |-4| = 4. Conclusion : p = 0.095 → on accepte H0 (pas de différence significative), mais résultat limite (proche du seuil). "],["tests-statistiques-anova-et-kruskal-wallis.html", "Chapter 4 Tests statistiques (ANOVA et Kruskal Wallis) 4.1 Tests statistiques pour plusieurs groupes 4.2 Tests statistiques Multiples. 4.3 Révisions 4.4 Travaux pratiques", " Chapter 4 Tests statistiques (ANOVA et Kruskal Wallis) 4.1 Tests statistiques pour plusieurs groupes Pour ce cours nous prendrons un autre exemple concernant le potentiel de stockage du carbon au niveau du sol en fonction des types de climats/territoire. Ce travail a fait l’objet d’une publication scientifique dans le Journal PNAS. Vous pouvez lire le papier ici. Les données peuvent être téléchargées ici. Importer les données avec le code suivant : load(file=&quot;data/Carbon-Storage.rda&quot;) colnames(Carbon_storage) ## [1] &quot;Biome&quot; &quot;Type&quot; &quot;Current&quot; &quot;Potential&quot; La matrice présente quatre variables, une variable sur le type de climat/biome sur lequel a été évalué le stockage de carbon, la seconde variable correspond au type de terrain où a été réalisé le prélévement (par exemple, forêt ou terrain agricôle). Enfin, la troisième variable correspond au stockage de carbon actuel et la dernière variable concerne le stockage potential. La somme des deux dernières colonnes correspond donc à la capacité du biome à stocker du carbon. Dans les démonstrations qui vont suivre, nous nous intéresserons uniquement à la capacité actuelle et au trois biomes suivant : “Subtropical”, “Temperate” et “Boreal”. Sélectionner ces trois types de biome et la variable “Current”. Enregister le tout dans un nouvel objet. Représenter la capacité actuelle en fonction des 3 biomes. w &lt;- which(Carbon_storage$Biome==&quot;Subtropical&quot;|Carbon_storage$Biome==&quot;Polar&quot;|Carbon_storage$Biome==&quot;Boreal&quot;) df &lt;- data.frame(&quot;Biome&quot;= Carbon_storage$Biome,&quot;Current&quot;=Carbon_storage$Current)[w,] df = droplevels.data.frame(df) boxplot(Current~Biome, df, outline=F, col=c(&quot;darkgreen&quot;, &quot;tomato&quot;, &quot;gold&quot;)) Figure 4.1: Figure 1 : Représentation du potentiel de stockage en fonction des biomes. La question de recherche est maintenant de savoir s’il y a une différence dans le stockage de carbon (mesuré en 2022) entre ces trois biomes. De la même manière que les tests statistiques que nous avons vu dans les cours 2 et 3, nous devons verifier en amont si la variable pour chacun des groupes respecte la normalité et une variance égale. À l’aide de la fonction shapiro.wilk() et bartlett.test(), vérifier la normalité de la variable dans les trois biomes et si leurs variances sont proches. for(biome in unique(df$Biome)){ SW &lt;- shapiro.test(df$Current[which(df$Biome==biome)]) print(paste(&quot;Le test de Shapiro.Wilk renvoie une statistique de&quot;, round(SW$statistic, 2), &quot;et un p value associée de&quot;, SW$p.value, &quot;pour le biome suivant&quot;, biome)) } ## [1] &quot;Le test de Shapiro.Wilk renvoie une statistique de 0.26 et un p value associée de 1.41832946008063e-44 pour le biome suivant Subtropical&quot; ## [1] &quot;Le test de Shapiro.Wilk renvoie une statistique de 0.1 et un p value associée de 6.05259801465719e-26 pour le biome suivant Boreal&quot; ## [1] &quot;Le test de Shapiro.Wilk renvoie une statistique de 0.25 et un p value associée de 7.58129235692607e-14 pour le biome suivant Polar&quot; Que conclure sur la normalité de la variable en fonction des groupes ? bartlett.test(Current~Biome, df) ## ## Bartlett test of homogeneity of variances ## ## data: Current by Biome ## Bartlett&#39;s K-squared = 3028.6, df = 2, p-value &lt; 2.2e-16 Que conclure sur l’égalité des variance de la variable entre les groupes ? Vers quelle famille de test statistique allez-nous nous tourner pour répondre à notre question biologique ? 4.1.1 Test non paramétrique (Kruskall-Wallis) Le test de Kruskall-Wallis permet de comparer la position de 3 échantillons ou plus, \\(x_1 = \\{x_{11},··· ,x_{1n_1}\\} , ···, x_k = \\{x_{k1},··· ,x_{kn_k}\\}\\). \\(H_0 : \\hat{\\theta}_1 =\\hat{\\theta}_2=\\hat{\\theta}_k\\) contre au moins une des \\(\\theta\\) est différent des autres (\\(H_1\\)). Pour rappel \\(\\theta\\) est un paramètre, ici la distribution d’une variable pour un groupe \\(k\\). Le test de Kruskall-Wallis fait une analyse de la variance non paramétrique. On l’utilise pour les petits échantillons ou quand l’hypothèse de normalité n’est pas vérifiée. La statistique de test est donnée par : \\[S_{KW}= \\frac{12}{n(n+1)} \\sum_{i=1}^k \\frac{R_{i.}^2}{n_i}-3(n+1)\\] où \\(n=\\sum_{i=1}^k n_i\\) est le nombre total d’observation et \\(R_{i.}\\) est la somme des rangs de l’échantillon \\(i\\). La loi de la statistique de test est tabulée pour les petits échantillons. Pour les grands échantillons, la statistique de test suit une loi du \\(\\chi^2\\) à$ k − 1$ degrés de liberté́. En présence, d’ex aequo, il existe une formule ajustée. kruskal.test(Current~Biome, df) ## ## Kruskal-Wallis rank sum test ## ## data: Current by Biome ## Kruskal-Wallis chi-squared = 12.314, df = 2, p-value = 0.002119 Que pouvez vous dire par rapport aux résultats du test statistique et à la probabilité d’observer ce résultat ? 4.1.2 Test paramétrique (ANOVA) 4.1.2.1 Normalisation Nous ne pouvons pas utiliser les précédentes données pour un test statistique À MOINS DE TRANSFORMER CES DONNÉES, de les normaliser. La transformation des données permet en outre de pouvoir utiliser des tests statistiques paramétriques (qui possèdent une puissance plus importante que les tests non paramétriques). Plusieurs transformations existent. Nous en verrons deux à travers ce cours. La normalisation en centrant et réduisant les données (nous verrons cela lors du prochain cours) et la normalisation par transformation des données selon une échelle logarithmique (i.e. la fonction log() dans R). boxplot(log(Current)~Biome, df, col=c(&quot;darkgreen&quot;, &quot;tomato&quot;, &quot;gold&quot;)) Figure 4.2: Figure 2 : Représentation du stockage actuelle (log transformé) en fonction des biomes. Quelles sont vos remarques ? Refaire le test de normalité et d’égalités des variances. for(biome in unique(df$Biome)){ SW &lt;- shapiro.test(log(df$Current[which(df$Biome==biome)])) print(paste(&quot;Le test de Shapiro.Wilk renvoie une statistique de&quot;, round(SW$statistic, 2), &quot;et un p value associée de&quot;, SW$p.value, &quot;pour le biome suivant&quot;, biome)) } ## [1] &quot;Le test de Shapiro.Wilk renvoie une statistique de 0.98 et un p value associée de 3.38726641706307e-06 pour le biome suivant Subtropical&quot; ## [1] &quot;Le test de Shapiro.Wilk renvoie une statistique de 0.99 et un p value associée de 0.66406346581351 pour le biome suivant Boreal&quot; ## [1] &quot;Le test de Shapiro.Wilk renvoie une statistique de 0.98 et un p value associée de 0.484560225022285 pour le biome suivant Polar&quot; bartlett.test(log(Current)~Biome, df) ## ## Bartlett test of homogeneity of variances ## ## data: log(Current) by Biome ## Bartlett&#39;s K-squared = 5.7155, df = 2, p-value = 0.0574 Que pouvons-nous conclure ? Attention il ne faut pas uniquement regarde la probabilité d’observée cet événement (p.value), la statistique est très importante et aide à prendre des décisions. 4.1.2.2 ANOVA à effet fixe (1 facteur) Pour comprendre l’ANOVA (ANalysis Of VAriance), il faut bien caractériser les variables du problème pour ne pas faire d’erreur dans la formule. Le Biome est une variable considérée comme qualitative avec trois modalités bien déterminées. Nous l’appelons le facteur. Ici, le facteur ” Biome ” est à effets fixes. Le stockage actuel du Carbone au niveau des sol est considérée comme quantitative car obtenus par une mesure (analyse d’image). Nous l’appelons la variable réponse. L’analyse de la variance permet de répondre à la question : “Y a-t-il un effet biome sur les stockages de carbone au niveau des sol ?” L’analyse de la variance (ANOVA) est une méthode statistique qui permet d’étudier la modification de la moyenne \\(\\mu\\) d’une quantité \\(Y\\) (variable réponse quantitative) selon l’influence éventuelle d’un ou de plusieurs facteurs d’expérience qualitatifs (ici biome) : le facteur \\(X\\). Le facteur X est souvent une variable qualitative présentant un nombre restreint de modalités. On note \\(J\\) le nombre de modalités (ici \\(J=3\\)). Le modèle de l’analyse de la variance s’écrit : $ Y_{i,j} = + j+{i,j} $ pour \\(i = 1, …, I\\) et \\(j = 1, …, J\\) où \\(Y_{i,j}\\) est la valeur de la variable réponse \\(Y\\) (i.e. sockage de carbon) pour l’observation \\(i\\) dans la modalité (niveau, i.e. biome) \\(j\\). \\(\\mu\\) est l’effet moyen. \\(\\alpha_j\\) est l’effet différential de la modalité \\(j\\) (i.e. d’un biome). \\(\\varepsilon_{i,j}\\) est le terme d’erreur aléatoire. cnt=1 boxplot(log(Current)~Biome, df, col=c(&quot;darkgreen&quot;, &quot;tomato&quot;, &quot;gold&quot;)) abline(h=mean(log(df$Current)), col=&quot;cornflowerblue&quot;, lwd=2, lty=&quot;dashed&quot;) for(biome in unique(df$Biome)){ points(x=as.numeric(unique(df$Biome))[cnt],y=mean(log(df$Current[which(df$Biome==biome)])), pch=16, cex=1, col=&quot;red&quot;) cnt=cnt+1 } Figure 4.3: Figure 3 : Représentation graphique du modèle de l’Analyse des Variances. La ligne en blue correspond à la moyenne globale du modèle. Les points rouges correspondent à la somme de la moyenne globale + l’effet différentiel (moyenne des groupes). Estimation des paramètres du modèle L’estimation des paramètres sur R se réalise grâce à la fonction lm(). La première qui consistait à savoir quelle variable était Y et laquelle était X est très importante puisqu’elle permet de bien écrire le modèle. Ce que nous tenter de modéliser, c’est bien le stockage de carbone actuel en fonction de la variable fixe : biome. Dans R, le mot “en fonction” s’écrit ~. Ainsi la formule dans R sera : summary(lm(log(Current)~Biome, data=df)) ## ## Call: ## lm(formula = log(Current) ~ Biome, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.1024 -2.3482 0.2012 2.6407 10.3311 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.9508 0.3064 52.061 &lt;2e-16 *** ## BiomePolar 1.0181 0.6312 1.613 0.1072 ## BiomeSubtropical -0.8022 0.3395 -2.363 0.0184 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.702 on 829 degrees of freedom ## Multiple R-squared: 0.01691, Adjusted R-squared: 0.01454 ## F-statistic: 7.13 on 2 and 829 DF, p-value: 0.000851 Décortiquer les résultats du modèle. Identifier la moyenne de la variable en fonction des groupes ? L’intercept dans ce modèle est le biome “Boreal”. Mais non pouvons écrire un modèle sans intercept. summary(lm(log(Current)~0+Biome, data=df)) ## ## Call: ## lm(formula = log(Current) ~ 0 + Biome, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.1024 -2.3482 0.2012 2.6407 10.3311 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## BiomeBoreal 15.9508 0.3064 52.06 &lt;2e-16 *** ## BiomePolar 16.9688 0.5519 30.75 &lt;2e-16 *** ## BiomeSubtropical 15.1486 0.1462 103.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.702 on 829 degrees of freedom ## Multiple R-squared: 0.9455, Adjusted R-squared: 0.9453 ## F-statistic: 4796 on 3 and 829 DF, p-value: &lt; 2.2e-16 Décortiquer les résultats du modèle. Identifier la moyenne de la variable en fonction des groupes ? Test d’hypothèse L’analyse de la variance est souvent utilisée pour tester l’égalité des moyennes. Hypothèses de test \\(H_0 : \\mu_1 = \\mu_2 = … = \\mu_J\\) contre \\(H_1\\) : au moins une des moyennes est différente. ou bien (de manière équivalente) : \\(H_0 : \\alpha_1 = \\alpha_2 = … = \\alpha_J = 0\\) contre \\(H_1\\) :au moins un \\(\\alpha_j\\) est différent de zéro. On pourrait dire pour notre exemple de biome : Les trois biomes stocke la même quantité de carbone (en moyenne). Les effets différenciels des trois biomes sont nuls. Rejeter \\(H_0\\) s’interprète de la manière suivante : (1) Deux niveaux différents du facteur \\(X\\) entrainent une différence significative de la variable réponse \\(Y\\). (2) Le facteur \\(X\\) a un effet significatif sur la variable réponse \\(Y\\). Statistique de l’ANOVA La statistique de test se base sur les différences observées entre les moyennes des échantillons (variance inter-groupe) et les différences entre les observations à l’intérieur des échantillons (variance intra-groupe). On s’intéresse au rapport entre la variance expliquée par les facteurs (ici biome) et la variance des résidus intra-groupe. Dans ce cas, nous prendrons comme notation : SCT (Somme des Carrés Totale) traduit la variation totale de Y. SCE (Somme des Carrés Expliquée) traduit la variation expliquée par le modèle. SCR (Somme des Carrés Résiduelle) traduit la variation inexpliquée par le modèle. et nous avons la relation : \\[SCT=SCE+SCR\\] ylim=range(log(df$Current)) layout(matrix(c(1,2,3,3,3),nrow=1)) par(mar=c(4,4,3,2)) cnt=1 boxplot(log(df$Current), ylim=ylim, main=&quot;Variance totale (SCT)&quot;, ylab=&quot;Carbon Storage&quot;) abline(h=mean(log(df$Current)), col=&quot;cornflowerblue&quot;, lwd=1, lty=&quot;dashed&quot;) plot(x=1, y=1, col=&quot;white&quot;, ylim=ylim, ylab=&quot;Carbon Storage&quot;, axes=F, xlab=&quot;&quot;, main=&quot;Variance intergroupe (SCE)&quot;) for(biome in unique(df$Biome)){ points(x=1,y=mean(log(df$Current[which(df$Biome==biome)])), pch=16, cex=1, col=&quot;red&quot;) cnt=cnt+1 } axis(2) box() abline(h=mean(log(df$Current)), col=&quot;cornflowerblue&quot;, lwd=1, lty=&quot;dashed&quot;) plot(x=as.numeric(df$Biome), y=log(df$Current), ylim=ylim, xlim=c(0.5, 3.5), ylab=&quot;Carbon Storage&quot;, axes=F, xlab=&quot;&quot;, main=&quot;Variance intragroupe (SCR)&quot;) cnt=1 for(biome in unique(df$Biome)){ segments(x0=as.numeric(unique(df$Biome))[cnt]+0.1, y0=mean(log(df$Current[which(df$Biome==biome)]))-sd(log(df$Current[which(df$Biome==biome)])), y1=mean(log(df$Current[which(df$Biome==biome)]))+sd(log(df$Current[which(df$Biome==biome)])), col=&quot;green3&quot;, lwd=2) cnt=cnt+1 } axis(2) box() abline(h=mean(log(df$Current)), col=&quot;cornflowerblue&quot;, lwd=1, lty=&quot;dashed&quot;) Figure 4.4: Figure 4 : Représentation graphique du modèle de l’Analyse des Variances avec la variance totale, la variance intergroupe et la variance intragroupe. La ligne en blue correspond à la moyenne globale du modèle. Les points rouges correspondent à la somme de la moyenne globale + l’effet différentiel (moyenne des groupes). Les lignes vertes correspondent à la variance intragroupe. On utilise la statistique de test \\[F_n = \\frac{SCE/(J − 1)}{SCR/(n − J)}\\] SCE/(J − 1) est la part de variance de \\(Y\\) expliquée par le modèle (i.e. par les modalités/facteurs \\(X\\)), SCR/(n − J) est la part résiduelle de variance \\(Y\\). Plus \\(F_n\\) est grande et plus on s’écarte de l’hypothèse nulle. Autrement plus la distance entre les points rouge augmente ET/OU la variance résiduelle (intragroupe) diminue, plus grande sera la statistique F. # ANOVA model_estimation &lt;-lm(log(Current)~Biome, data=df) anova(model_estimation) ## Analysis of Variance Table ## ## Response: log(Current) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Biome 2 195.4 97.714 7.1297 0.000851 *** ## Residuals 829 11361.7 13.705 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La p value du test est très petite devant 5% d’erreur, nous pouvons dire qu’il existe un effet significatif de la modalité (biome) sur la variable réponse \\(Y\\) (stockage de carbone). 4.1.2.3 ANOVA à effet fixe (2 facteurs) EN TP. 4.2 Tests statistiques Multiples. 4.2.1 Généralités Si plusieurs hypothèses sont testées, la probabilité́ d’un évènement rare augmente et, par conséquent, la probabilité́ de rejeter de manière incorrecte une hypothèses nulle (c’est-à-dire commettre une erreur de type I) augmente. Dans ce cas là, l’hypothèse nulle rejetée serait un faux résultat positif. La correction de Bonferroni compense cette augmentation en testant chaque hypothèse à un niveau de signification (ou de risque) de \\(\\alpha /k\\) ou \\(\\alpha\\) est le niveau global souhaité et \\(k\\) le nombre d’hypothèses. Dans l’exemple précèdent, on a 3 groupes et donc \\(k = 3\\). Si on fait les comparaisons multiples au risque 5%, on doit comparer chacune des p.values à 0.05/3. Le nouveau seuil de significativité serait 0.017. Sinon, cela revient à multiplier les p-values par k (\\(k=3\\) dans l’exemple) et comparer les p-value « corrigées » au seuil initialement fixé (par ex. 0.05). Aussi appelé test de Dunn. 4.2.2 Applications Dans le cas de nos Biome, nous souhaitons les comparer 2 à 2 et savoir comment ils diffèrent entre eux par rapport à leur capacité à stocker du Carbon. 1) Correction de Bonferroni L’idée est de réaliser des tests wilcoxon entre deux biomes et estimer la p.value. for(i in unique(df$Biome)[1:2]){ for(j in unique(df$Biome)[2:3]){ if(i!=j){ X &lt;- df$Current[which(df$Biome==i)] Y &lt;- df$Current[which(df$Biome==j)] print(paste(i, &quot;Versus&quot;, j)) print(wilcox.test(X, Y)$p.value) } } } ## [1] &quot;Subtropical Versus Boreal&quot; ## [1] 0.03237358 ## [1] &quot;Subtropical Versus Polar&quot; ## [1] 0.002687245 ## [1] &quot;Boreal Versus Polar&quot; ## [1] 0.1226622 Entre quelles biomes observons nous des capacités de stockages différentes ? Même question en applicant la correction de Bonnferroni ? 2) Test de Dunn On applique directement la correction de Dunn sur le script de notre boucle en multipliant la p.value de chaque test par le nombre de tests réalisés (ici \\(k=3\\)). for(i in unique(df$Biome)[1:2]){ for(j in unique(df$Biome)[2:3]){ if(i!=j){ X &lt;- df$Current[which(df$Biome==i)] Y &lt;- df$Current[which(df$Biome==j)] print(paste(i, &quot;Versus&quot;, j)) print(wilcox.test(X, Y)$p.value*length(unique(df$Biome))) } } } ## [1] &quot;Subtropical Versus Boreal&quot; ## [1] 0.09712074 ## [1] &quot;Subtropical Versus Polar&quot; ## [1] 0.008061734 ## [1] &quot;Boreal Versus Polar&quot; ## [1] 0.3679865 Entre quelles biomes observons nous des capacités de stockages différentes ? 4.3 Révisions 4.3.1 QCM 1. L’ANOVA se réalise lorsqu’il y a une variable quantitative et uniquement une variable qualitative. - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux 2. Quelles phrases sont correctes ? - [ ] On accepte H0 lorsque la p value est inférieure à 0.05 - [ ] On rejette H1 lorsque la p value est inférieure à 0.05 - [ ] Le seuil de significativité est modifié lorsque vous utilisez la correction de Dunn sur des tests multiples - [ ] Le Kruskal-Wallis est un test non paramétrique Afficher la réponse ✅ On rejette H0 lorsque la p value est inférieure à 0.05. ✅ Le seuil de significativité est modifié avec la correction de Dunn. ✅ Le Kruskal-Wallis est un test non paramétrique. 3. L’ANOVA à 2 facteurs se réalise lorsque le problème inclut 2 variables quantitatives et 1 variable qualitative. - [ ] Vrai - [ ] Faux Afficher la réponse ❌ Faux (L’ANOVA à 2 facteurs se réalise avec 2 variables qualitatives et 1 variable quantitative) 4. Combien d’\\(H_0\\) l’ANOVA à 2 facteurs peut-elle tester ? - [ ] 1 - [ ] 2 - [ ] 3 - [ ] 4 Afficher la réponse ✅ 3 4.3.2 Questions ouvertes (définitions) Définissez H0 (Hypothèse Nulle) lorsque vous réalisez un test du Shapiro-Wilk. Afficher la réponse H0 : La variable étudiée suit une loi normale. Quelle est la différence entre variable qualitative et modalité ? Afficher la réponse La variable qualitative est une variable catégorielle qui décrit une qualité/attribut. Une modalité est l’une des catégories que peut prendre cette variable. Exemple : Variable = « couleur », modalités = rouge, bleu, vert. Expliquez de manière vulgarisée, en vous appuyant sur la formule de l’ANOVA à 1 facteur, comment elle fonctionne. Afficher la réponse Le numérateur de la statistique de test correspond à la variabilité entre les moyennes des groupes. Le dénominateur correspond à la variabilité intra-groupe (résiduelle). La statistique augmente si : Les moyennes des groupes sont très différentes. Les dispersion intra-groupes est faible (faible écart-type). 4.3.3 Problème Les auteurs d’une publication (Am J Cancer Res 2017;7(5):1037-1053) testent deux variables qualitatives avec deux modalités chacune : Santé : CTL (contrôle) vs LS (cancéreux) Environnement : Inactive vs Active Ils mesurent la masse du tissu adipeux viscéral (moyenne ± SEM). Les résultats sont disponibles sur cette figure : Figure from Am J Cancer Res (2017) Décrire le graphique. Afficher la réponse Le graphique compare la masse adipeuse des souris selon une condition de santé (contrôle ou cancéreuse) et selon l’activité (active ou inactive). Visuellement, il semblerait qu’il y ait des différences entre les groupes. À partir du graphique, quels tests préconisez-vous ? Afficher la réponse Une ANOVA à 2 facteurs (santé × environnement). Donner les \\(H_0\\) associées à ce graphique (questions biologiques). Afficher la réponse \\(H_0\\) (1) La masse du tissu adipeux est identique quelle que soit la modalité de la variable environnementale (active vs inactive). \\(H_0\\) (2) La masse du tissu adipeux est identique quelle que soit la modalité de la variable santé (contrôles vs cancéreuses). \\(H_0\\) (3) La masse du tissu adipeux n’est pas influencée par l’interaction entre santé et environnement. Au vu du graphique, quelles \\(H_0\\) rejetteriez-vous ? Afficher la réponse On rejette \\(H_0\\) (2) → la masse adipeuse est différente selon la santé (contrôle vs cancéreuse). 4.4 Travaux pratiques Durée : 3h Nous utiliserons pour l’ensemble des prochains TPs un jeu de données issu d’une étude réalisée dans la Station Spatial Internationale. L’ensemble des résultats est déjà publiées dans cet article : https://link.springer.com/article/10.1007/s12217-018-9653-2. L’ensemble des données est téléchargeable sur la plateforme de la NASA : https://osdr.nasa.gov/bio/repo/data/studies/OSD-546. rm(list=ls()) # Permet de supprimer les objects et fonctions enregister sur l&#39;environnement. Objectifs : explorer les conséquences de la microgravité (i.e. pas de pesanteur) sur le métabolisme du fer dans des Hematopoïetic Stem Cell en condition de différenciation cellulaire (HSC vers osteoblastes). Les données du TP peuvent être téléchargées ici Importer les données df &lt;- read.table(file = &quot;data/TPs/iron_metabolism_RNA_TP.csv&quot;, sep=&quot;;&quot;, header = T) load(&quot;data/TPs/meta_data.rda&quot;) Définir les 4 conditions expérimentales. Montrer la matrice de confusion table(meta_data[,2:3]) ## Micro_env ## Macro_env osteogenic standard ## Ground 3 3 ## Space 3 4 Définir les conditions de ground versus Space ? Définir les conditions de osteogenic versus Standard. Lire le papier pour être au clair avec les conditions. Faire un petit récapitulatif de comment fonctionne le métabolisme du fer au niveau intra-cellulaire. Nous pourrons appuyer nos résultats sur de la littérature existante : https://faseb.onlinelibrary.wiley.com/doi/10.1096/fj.202301184R définir n et p. n &lt;- length(meta_data$ID) # Nombre d&#39;individus p &lt;- dim(df)[1] # Nombre de variables 4.4.1 Statistiques descriptives du metabolisme du fer Visualiser l’effet du macro-environnement sur chaque expression d’ARNm. layout(matrix(c(1:p), nrow = 4, byrow=T)) par(mar=c(2,4,2,2)) for(i in 1:p){boxplot(as.numeric(df[,-c(1,2)][, meta_data$ID][i,])~as.factor(meta_data$Macro_env), main=(df$SYMBOL)[i], xlab=&quot;&quot;, ylab=&quot;ARN expressions&quot;)} Que peut-on dire ? Quelles seraient les hypothèses H_O et H_1 ? Les données sont-elles normales et/ou possèdent une variance similaire entre les groupes ? Visualiser l’effet du micro-environnement sur chaque expression d’ARNm. layout(matrix(c(1:p), nrow = 4, byrow=T)) par(mar=c(2,4,2,2)) for(i in 1:p){boxplot(as.numeric(df[,-c(1,2)][, meta_data$ID][i,])~as.factor(meta_data$Micro_env), main=(df$SYMBOL)[i], xlab=&quot;&quot;, ylab=&quot;ARN expressions&quot;)} Que peut-on dire ? Quelles seraient les hypothèses H_O et H_1 ? Les données sont-elles normales et/ou possèdent une variance similaire entre les groupes ? Réaliser des correlations entre les variables pour vérifier le fonctionnement du métabolisme du fer. Faire la même figure en ajoutant des couleurs pour les conditions. 4.4.2 Tests Statistiques Quels tests statistiques utiliser ? via une boucle for() réaliser l’ensemble des tests ? pour déterminer l’effet de la vit D3 sur l’expression des ARNm ? pour déterminer l’effet de la microgravité sur l’expression des ARNm ? Quelle est la probabilité de faire une erreur de type I ? Appliquer une correction ? Dunn ou Bonferroni ? Que peut-on dire de l’effet de la VIT D3 ? Que peut-on dire de l’effet de la microgravité (espace) sur le métabolisme du fer dans les HSC/osteoblastes ? 4.4.3 Modelisation à 2 facteurs Peut-on utiliser l’AnOVa à 1 facteur ? Comprendre comment fonctionne l’AnOVa à 2 facteurs ? Ainsi que l’interaction ? Explorer les données en utilisant l’AnOVa à 2 facteurs ? Conclure. "],["tests-statistiques-corrélations.html", "Chapter 5 Tests statistiques (Corrélations) 5.1 Tests d’indépendance entre 2 variables quantitatives. 5.2 Paramètres des tests de correlation : Exemple avec la méthode de Pearson 5.3 Test de Spearman", " Chapter 5 Tests statistiques (Corrélations) Ce chapitre concerne cette fois ci les tests à utiliser lorsque 2 variables sont quantitatives. Nous illustrerons nos propos par un jeu de données caractérisant les résultats d’un heptathlon féminin. Le nombre d’athlètes s’élève à 18 (\\(n = 18\\)) et le nombre d’épreuve, logiquement à 8 (\\(p=8\\)). Le nom des lignes correspond au noms des athlètes. Vous pouvez télécharger les données ici. Commençons par visualiser le tableau de données. La fonction head() permet de visualiser les premières lignes d’un tableau. df &lt;- read.table(&quot;data/heptathlon_2023.csv&quot;, sep=&quot;,&quot;, header=T, stringsAsFactors = T, row.names = 1) head(df) ## Pays Points X100m Hauteur Poids X200m Longueur Javelot X800m ## Oosterwegel NED 6394 13.46 1.75 13.80 24.48 6.18 56.66 2m13s62 ## Kalin SUI 6390 13.21 1.75 12.33 24.20 6.67 46.05 2m15s20 ## Brooks USA 6181 12.91 1.75 12.93 24.02 6.24 37.41 2m15s66 ## Kunz USA 6126 13.36 1.72 13.54 24.43 5.96 44.68 2m15s98 ## Esselink NED 6101 13.36 1.75 13.28 24.63 5.82 46.75 2m17s68 ## Weissenberg GER 6055 13.72 1.81 13.67 24.05 6.18 43.42 2m32s65 5.1 Tests d’indépendance entre 2 variables quantitatives. 5.1.1 Généralités Objectif des tests d’association : estimer une valeur statistique qui résume la relation entre deux variables que nous nommerons ici \\(X\\) et \\(Y\\) dans un échantillon donné. Pour rappel, un échantillon n’est pas toujours représentatif de la population. Précédemment, on a mesuré l’association entre 2 variables qualitatives avec le test de \\(\\chi^2\\). Ici on testera l’association entre deux variables quantitatives par un test de corrélation. On mesure statistiquement la force de la relation entre 2 variables quantitatives dans un échantillon donnée (et uniquement entre 2 variables). Les associations entre plusieurs variables sont possibles, mais ne seront pas abordées dans ce chapitre. Pour rappel, pas de lien de cause à effet !! JUSTE UNE ASSOCIATION. Quelques tests statistiques : Le coefficient de corrélation de Pearson suppose une relation linéaire (PARAMETRIQUE) Le coefficient de corrélation de Spearman suppose une relation pas forcément linéaire (basée sur les rangs) et donc NON PARAMÉTRIQUE. Visuellement, on peut se baser sur un diagramme de dispersion bivarié et décrire l’ellipse entourant le nuage de points, plus elle est étroite, plus la relation linéaire sera importante. Dans l’exemple ci contre, je vous montre deux relations : (1) entre le temps au 100mH et le temps au 200m, (2) entre la performance au saut en longueur et le temps au 200m. layout(matrix(c(1,2), nrow=1)) plot(df$X100m, df$X200m, xlab=&quot;100H (s)&quot;, ylab=&quot;200 (s)&quot;, pch=16, col=&quot;gray&quot;) plot(df$Longueur, df$X200m, xlab=&quot;Longueur (m)&quot;, ylab=&quot;200 (s)&quot;, pch=16, col=&quot;gray&quot;) Que pouvez vous dire des relations entre les variables ? Est ce que l’on pourrait dire que la vitesse de course est proportionnelle à la longueur de saut ? 5.1.2 Conditions De la même manière que pour les test précédents, il est nécéssaire de vérifier la normalité des variables via un test du shapiro wilk. Si les 2 variables sont normales alors vous pourrez réaliser soit un test de Pearson (paramétrique) sinon vous devrez faire un test de Spearman (non paramétrique). Pour rappel, les tests paramétriques ont plus de puissance que les tests non paramétriques. Si les variables sont normales, il faut privilégiez les tests paramétriques. 5.2 Paramètres des tests de correlation : Exemple avec la méthode de Pearson Dans le test de Pearson, vous allez estimer plusieurs paramètres qui vont permettront de caractériser au mieux une dépendance entre deux variables. 5.2.1 Premier paramètre : \\(a\\), coefficient linéaire La fonction lm dans R est très importante. Dans notre cas elle permet de construire la fonction affine \\(f(x) = b + ax\\) où \\(a\\) est le coefficient linéaire et \\(b\\) est l’intercept. Vous pouvez ensuite l’afficher sur le graphique. lm(df$X200m~df$X100m) ## ## Call: ## lm(formula = df$X200m ~ df$X100m) ## ## Coefficients: ## (Intercept) df$X100m ## 20.0347 0.3369 Dans l’exemple de la relation entre le temps au 200m et le temps au 100mH, la fonction affine coupe l’axe des abscisse à 20.03” c’est le \\(b\\). Alors que l’indice \\(a\\) est de 0.33”. Cela signifie qu’une augmentation de 1” sur 100mH est associé à une augmentation de 0.33” au 200m. Nous pouvons représenter la fonction affine sur le graphique. plot(df$X100m, df$X200m, xlab=&quot;100H (s)&quot;, ylab=&quot;200 (s)&quot;, pch=16, col=&quot;gray&quot;) points(df$X100m[which(df$X100m&gt;15)],df$X200m[which(df$X100m&gt;15)], col=&quot;red&quot;, pch=16) abline(coef(lm(df$X200m~df$X100m))) Que pouvons nous dire de la relation ? Vous semble t’elle représenter la relation du plot ? Que peut-on dire du point rouge ? Refaire le graphique en enlevant le point rouge ? Que pouvons-nous dire ? 5.2.2 Second paramètre : \\(r\\) \\(r\\) est le coefficient de correlation qui mesure la force estimée de la relation linéaire entre 2 variables de l’échantillon. Possibilité d’obtenir manuellement le \\(r\\) à partir des données brutes (X et Y) via la covariance. La covariance est le produit moyen des distances entre les observations et leurs moyennes respectives, soit : \\[ r_{X,Y} = \\frac{Cov(X, Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}\\] avec \\(Cov(X, Y) = \\frac{1}{N-1} \\sum_{i=1}^N{(X_i-\\hat{\\mu}_X)(Y_i-\\hat{\\mu}_Y)}\\) et \\(Var(X) = \\frac{1}{N-1} \\sum_{i=1}^N{(X_i-\\hat{\\mu}_X)^2}\\) ou Ecart type \\(X=\\sqrt{Var(X)}\\). La covariance permet de répondre à la question : Est-ce que Y varie quand X varie (dépendance) ? Auquel cas, est ce que Y varie dans le même sens que X (quelle direction, définie par le signe de la relation) ? Si association positive : une observation i avec une valeur de xi supérieure à la moyenne de X devrait également avoir une valeur de yi supérieure à la moyenne de Y. Prenons le temps au 100mH comme X et le temps au 200m comme Y. Il faut bien définir la variable Réponse (i.e. \\(Y\\)) ici car elle vous guidera durant les calculs. Calculons la \\(Cov(X, Y)\\) n &lt;- dim(df)[1] x_center &lt;- df$X100m-mean(df$X100m) y_center &lt;- df$X200m-mean(df$X200m) Covariance &lt;- sum(x_center*y_center)/(n-1) Calculons la \\(\\sqrt{Var(X)}\\) puis la \\(\\sqrt{Var(Y)}\\) var_x &lt;- sqrt(sum(x_center^2)/(n-1)) var_y &lt;- sqrt(sum(y_center^2)/(n-1)) Calculons le coefficient de correlation \\(r_{X,Y}\\). r &lt;- Covariance/(var_x*var_y) print(r) ## [1] 0.2843454 Que pouvons-nous dire de cette relation ? un \\(r\\) de 0.28 vous semble t’il bien refléter la relation ? Refaire le calcul en enlevant l’individu rouge. Peu importe la méthode, un coefficient de corrélation se situe toujours entre -1 et 1 : -1 : forte relation linéaire négative. 0 : pas de relation linéaire. +1 : forte relation linéaire positive. 5.2.3 troisième paramètre : \\(r^2\\) Le \\(r^2\\) est le coefficient de détermination. Il mesure la variance expliquée par le modèle linéaire. Le \\(r^2\\) est forcément compris entre 0 et 1. Plus le \\(r^2\\) se rapproche de 1, meilleure est la dépendance entre \\(X\\) et \\(Y\\). L’intérêt d’un modèle de régression linéaire réside dans sa capacité à expliquer une partie des variations de la variable Y par les variations de la variable X. Elle mesure l’explication d’une variable par une autre. Pour calculer le coefficient de détermination, il suffit de reprendre les annotations de l’ANOVA (ANalyse Of VAriance). Dans ce cas, nous avons SCT (Somme des Carrés Totale) traduit la variation totale de Y = SCE (Somme des Carrés Expliquée) traduit la variation expliquée par le modèle + SCR (Somme des Carrés Résiduelle) traduit la variation inexpliquée par le modèle (c’est l’erreur du modèle (représenté par la droite), c’est à dire qu’il suffit de calculer la distance de chaque points avec le modèle (la droite)). On appelle coefficient de détermination la quantité suivante : \\(R^2=\\frac{SCE}{SCT}\\) ou bien \\(R^2=1-\\frac{SCR}{SCT}\\) model &lt;- lm(df$X200m~df$X100m) ## Nous ajustons un modèle linéaire où la variable réponse est le temps au 200m. SCT = sum(y_center^2) ## Nous calculons la somme des carrés totale de la variable réponse Y. SCE = sum(model$residuals^2) ## Puis la somme des carrés non expliquée de la variable réponse Y par X. r_squared &lt;- 1-(SCE/SCT) print(r_squared) ## [1] 0.08085229 Que pouvons nous dire du coefficient de détermination ? Est-il élevé ? P.S. : On peut aussi calculer le \\(r^2\\) juste en mettant au carré le coefficient de correlation \\(r\\). On retombe sur la même valeur. r^2 ## [1] 0.08085229 A retenir : l’objectif du modèle linéaire et de l’ensemble des modèles en général est de minimiser l’erreur et donc de minimiser la valeur des résidues associés au modèle (droite, polynôme du 2nd degrée, sinusoïdale, etc…). 5.2.4 quatrième paramètre : \\(p value\\) Inférence : est-ce que le coefficient de corrélation r est différent de 0 ? La statistique pour tester \\(H_0 : r = 0\\) est une statistique que nous avons déjà abordée : la statistique de student. Calcule de t (la statistique) à partir de r. \\[S_t=\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\] Si vous avez compris comment fonctionne les statistiques de tests : l’idée est de comparer l’éloignement entre \\(r=0\\) et \\(\\hat{r}\\). Dans le calcul de la statistique plus vous avez un \\(r\\) grand et plus la statistique sera différente de 0 (i.e. lorsque \\(r=0\\)) et donc vous pourrez refuser \\(H_0\\) et accepter \\(H_1\\). 5.2.5 Code R Je vous joins maintenant les fonctions R qui vous permettent de calculer de manière plus rapide l’ensemble des paramètres 1) Via un modèle linéaire model &lt;- lm(df$X200m~df$X100m) summary(model) ## ## Call: ## lm(formula = df$X200m ~ df$X100m) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.24480 -0.25603 -0.07956 0.39133 1.11427 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.0347 3.9153 5.117 0.000103 *** ## df$X100m 0.3369 0.2839 1.186 0.252802 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.59 on 16 degrees of freedom ## Multiple R-squared: 0.08085, Adjusted R-squared: 0.02341 ## F-statistic: 1.407 on 1 and 16 DF, p-value: 0.2528 2) Via la fonction de correlation cor.test(df$X200m,df$X100m, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: df$X200m and df$X100m ## t = 1.1864, df = 16, p-value = 0.2528 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2104643 0.6631771 ## sample estimates: ## cor ## 0.2843454 Que pouvons nous dire ? Refaire l’analyse sans le point rouge ? 5.3 Test de Spearman Pour le test de Spearman, c’est exactement la même chose sauf que la formule se base sur les rangs. \\[ r_{X,Y} = \\frac{Cov(Rang(X), Rang(Y))}{\\sqrt{Var(Rang(X))}\\sqrt{Var(Rang(Y))}}\\] Dans notre exemple, faut-il utiliser le test de Spearman ou bien de Pearson ? Il faut tester la normalité et l’égalité des variances. Vous pourrez utiliser la méthode de Spearman en modifiant l’argument method comme suit cor.test(X,Y, method=\"spearman\"). "],["clustering.html", "Chapter 6 Clustering 6.1 Généralités 6.2 K-means (ou algorithme des centres mobiles en français) 6.3 Classification Hiérarchique Ascendante 6.4 Définir le nombre de classes optimales ? 6.5 Travaux pratiques", " Chapter 6 Clustering Les objectifs de ce chapitre sont de comprendre les outils qui permettent de trouver des patterns d’individus via l’expression de leurs variables. Que peut-on dire du graphique ci dessous ? set.seed(1) X &lt;- data.frame(ASAT = c(rnorm(30, mean=0, sd=1), rnorm(30, 4, 1)), ALAT = c(rnorm(30, mean=0, sd=1), rnorm(30, 2, 1))) plot(X, xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;firebrick&quot;) Que pouvons nous remarquer ? Comment synthétiser l’information ? Ici, nous avons simulé 30 individus avec une moyenne de 0 pour la variable ASAT et la variable ALAT, alors qu’un autre groupe de 30 individus possède une moyenne de 4 au ASAT et de 2 au ALAT. Pour rappel ces deux transaminases reflètent des lésions hépatiques. 6.1 Généralités Définition des termes : Classification : Regrouper des individus en groupes ou classes d’individus proches. Non supervisée : dont on ne connait pas la réalité, on n’a pas d’information a priori. Autrement dit, la classification non supervisée (ou clustering) est la recherche d’une partition ou d’une répartition des individus en classes homogènes, de sorte à ce que celles-ci soient les plus distinctes possibles. Que souhaitons nous estimer ? L’appartenance à une classe pour chaque individu. Le nombre de classes optimal. Dans ce chapitre, nous verrons 2 algorithmes qui permettent de définir la classe des individus : la classification k-means et la Classification Hiérarchique Ascendante (HAC en anglais). 6.2 K-means (ou algorithme des centres mobiles en français) 6.2.1 Principe On demande à l’algorithme de nous donner un nombre \\(n_c\\) de groupes. C’est le nombre de groupe que l’on pourrait attendre. Dans un cas, nous pouvons faire l’hypothèse que la population peut se classer selon \\(n_c\\) patterns mais dans d’autres cas, nous ne savons quelle nombre de groupes choisir. L’algorithme doit rendre : \\(n_c\\) centres. A chaque point on associe le centre le plus proche, ce qui détermine les groupes, et des régions. Si on reprend le graphique de départ et que l’on donne deux groupes, on peut les placer de manière intuitif sur le graph. Chaque point sera associé à un groupe en fonction de sa distance aux centroïdes des groupes. 6.2.2 Algorithme du k-means (appartenance à une classe pour chaque individu) L’algorithme du k-means se base sur plusieurs étapes : Vous pouvez retrouver un exemple sur le lien suivant. Première étape (INITIALISATION): on prend au hazard un nombre de points égal au nombre de groupes que nous cherchons (ici \\(n_c = 2\\)). Nous allons prendre les points 24 et 4 (représenté en blue et jaune sur le graphique). plot(X, xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;firebrick&quot;) points(x=X[24,1], y=X[24,2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;cornflowerblue&quot;) points(x=X[4,1], y=X[4,2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;gold&quot;) Seconde étape (ASSIGNATION) : chaque point sera associé à l’un des points initialisés (bleus ou jaune) en fonction de leur distance par rapport à ce dernier. Pour rappel, ici nous utiliserons la distance euclidienne dont la formule pour deux variable x et y est \\(d(M_1, M_2) = \\sum{\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}}\\) où \\(M_i\\) correspond aux individus. ## ASSIGNATION d_n_blue &lt;- as.matrix(dist(rbind(X[24,],X)))[-1,1] # Distance de l&#39;ensemble des points au point bleu. d_n_gold &lt;- as.matrix(dist(rbind(X[4,],X)))[-1,1] # Distance de l&#39;ensemble des points au point jaune. cluster &lt;- apply(cbind(d_n_blue, d_n_gold),1,which.min) # phase d&#39;assignation. cluster_col &lt;- as.character(factor(cluster, c(1,2), c(&quot;cornflowerblue&quot;, &quot;gold&quot;))) plot(X, xlab = &quot;ASAT&quot;, ylab = &quot;ALAT&quot;, pch = 1, col = cluster_col, main = &quot;Distances des points par rapport aux points initiaux&quot;) points(x=X[24,1], y=X[24,2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;cornflowerblue&quot;) points(x=X[4,1], y=X[4,2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;gold&quot;) Dans ce graphique, les points pleins sont les individus sélectionnés lors de l’étape d’INITIALISATION, alors que les points vides sont les autres individus du jeu de données. La couleur de leur points sera en fonction de leur distance aux points pleins. Si l’individu se retrouve plus proche du points bleue alors il sera assigné à son groupe, et inversement pour les points du groupe jaune. Troisième étape (NOUVEAUX CENTROÏDES) : Dans cette troisième étape il faut recalculer le centroïde de chaque cluster (bleu et jaune), c’est-à-dire les centres de gravité du nuage de points bleue et du nuage de points jaunes. Ce seront nos nouveaux points de départ pour recommencer la phase d’ASSIGNATION. ## NOUVEAUX CENTROÏDES centroide_blue &lt;- colMeans(X[which(cluster==1),]) centroide_jaune &lt;- colMeans(X[which(cluster==2),]) plot(X, xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=1, col=&quot;firebrick&quot;) points(x=X[24,1], y=X[24,2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;cornflowerblue&quot;) points(x=X[4,1], y=X[4,2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=&quot;gold&quot;) points(centroide_blue[1], centroide_blue[2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=15, col=&quot;cornflowerblue&quot;) points(centroide_jaune[1], centroide_jaune[2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=15, col=&quot;gold&quot;) legend(&quot;topleft&quot;, c(&quot;1ere itération&quot;, &quot;2nd itération&quot;), bty=&quot;n&quot;, pch= c(16, 15), title = &quot;Centres de gravité (nuage de points)&quot;) Noter que les nouveaux centres de gravité ne sont plus associées à des individus en particulier. Boucler ces deux dernières étapes (ASSIGNATION + NOUVEAUX CENTROÏDES) pour converger vers des clusters homogènes. d_n_blue &lt;- as.matrix(dist(rbind(centroide_blue,X)))[-1,1] # Distance de l&#39;ensemble des points au carré bleu. d_n_gold &lt;- as.matrix(dist(rbind(centroide_jaune,X)))[-1,1] # Distance de l&#39;ensemble des points au carré jaune. cluster &lt;- apply(cbind(d_n_blue, d_n_gold),1,which.min) # phase d&#39;assignation. cluster_col &lt;- as.character(factor(cluster, c(1,2), c(&quot;cornflowerblue&quot;, &quot;gold&quot;))) layout(matrix(c(1,2), nrow=1)) plot(X, xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=1, col=cluster_col, main = &quot;ASSIGNATION&quot;) points(centroide_blue[1], centroide_blue[2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=15, col=&quot;cornflowerblue&quot;) points(centroide_jaune[1], centroide_jaune[2], xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=15, col=&quot;gold&quot;) ## NOUVEAUX CENTROÏDES centroide_blue_2 &lt;- colMeans(X[which(cluster==1),]) centroide_jaune_2 &lt;- colMeans(X[which(cluster==2),]) plot(X, xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=1, col=&quot;firebrick&quot;, main = &quot;NOUVEAUX CENTROÏDES&quot;) points(centroide_blue[1], centroide_blue[2],pch=15, col=&quot;cornflowerblue&quot;) points(centroide_jaune[1], centroide_jaune[2], pch=15, col=&quot;gold&quot;) points(centroide_blue_2[1], centroide_blue_2[2],pch=17, col=&quot;cornflowerblue&quot;) points(centroide_jaune_2[1], centroide_jaune_2[2], pch=17, col=&quot;gold&quot;) legend(&quot;topleft&quot;, c(&quot;2nd itération&quot;, &quot;3eme itération&quot;), bty=&quot;n&quot;, pch= c(15,17)) À la fin, il faut faire une dernière étape d’ASSIGNATION pour définir la classe des individus où les triangles sont les centroïdes respectives des deux groupes. ## ASSIGNATION FINALE d_n_blue &lt;- as.matrix(dist(rbind(centroide_blue_2,X)))[-1,1] # Distance de l&#39;ensemble des points au point bleu. d_n_gold &lt;- as.matrix(dist(rbind(centroide_jaune_2,X)))[-1,1] # Distance de l&#39;ensemble des points au point jaune. cluster &lt;- apply(cbind(d_n_blue, d_n_gold),1,which.min) # phase d&#39;assignation. cluster_col &lt;- as.character(factor(cluster, c(1,2), c(&quot;cornflowerblue&quot;, &quot;gold&quot;))) plot(X, xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=1, col=cluster_col) points(centroide_blue_2[1], centroide_blue_2[2],pch=17, col=&quot;cornflowerblue&quot;) points(centroide_jaune_2[1], centroide_jaune_2[2], pch=17, col=&quot;gold&quot;) On peut remarquer que l’algorithme converge bien vers les 2 groupes que l’on aurait pu imaginer en regardant le graphique au départ. On peut aussi utiliser l’algorithme k-means directement sur R, en appelant la function kmeans de la librairie stats. res.kmeans &lt;- kmeans(X, centers = 2, iter.max = 1, nstart = 1) # Utiliser le fonction help pour savoir à quoi corresponde les arguments de la fonction. res.kmeans ## K-means clustering with 2 clusters of sizes 30, 30 ## ## Cluster means: ## ASAT ALAT ## 1 4.13277458 2.113333 ## 2 0.08245817 0.110278 ## ## Clustering vector: ## [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 ## [39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 40.91834 51.51169 ## (between_SS / total_SS = 76.8 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Dans l’objet res.kmeans, on retrouve : les coordonnées des centroïdes en fonction des variables. res.kmeans$centers ## ASAT ALAT ## 1 4.13277458 2.113333 ## 2 0.08245817 0.110278 la classe des individus. res.kmeans$cluster ## [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 ## [39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 On retrouve bien les 30 premiers individus dans un groupe et les 30 derniers dans un autre groupe. C’est exactement de cette manière que nous avons simulé nos données. L’algorithme est donc performant pour retrouver les patterns des individus. 6.3 Classification Hiérarchique Ascendante La classification hiérarchique ascendante est basée sur un algorithme glouton. Au départ chaque individu forme une classe à lui seul. A chaque étape, les deux classes les plus proches sont agglomérées (critère local), pour finir avec une classe unique qui regroupe tous les individus. L’arbre obtenu est coupé a posteriori de façon à obtenir un bon compromis entre le nombre de classes et la variance intra-classe (critère global). Cet algorithme se base sur une distance entre les individus et sur une méthode d’agglomération. 6.3.1 Mesure d’éloignement entre les individus On parle de mesure de dissimilarités, c’est à dire à quel point les individus se ressemble ou bien s’éloigne en fonction du nombre de caractéristiques qu’ils ont en commun. Il existe plusieurs mesures de dissimilarités : Grover, Bray-Curtis, Jaccard, etc. A noter que la distance euclidienne peut aussi être utilisée. 6.3.2 Mesure d’agrégation Il en existe plusieurs, comme le saut minimum, la distance maximum, la moyenne ou encore la méthode de Ward. Chaque méthode génère un résultat différent. Nous n’aborderons pas ici les détails de ces techniques, mais la méthode de Ward est souvent privilégiée. Cette méthode vise à minimiser l’inertie à l’intérieur des classes et à maximiser celle entre les classes pour obtenir des groupes aussi homogènes que possible. La fonction principale pour calculer un dendrogramme est hclust, où l’on spécifie le critère d’agrégation avec l’option method. Dans notre cas, nous choisirons la méthode de Ward appliquée au carré des distances, en spécifiant method = \"ward.D2\". D &lt;- dist(X) # Distance entre les individus. HAC &lt;- hclust(D, method = &quot;ward.D&quot;) # Méthode d&#39;agglomération. layout(matrix(c(1,2), nrow=1)) plot(HAC) groupe &lt;- cutree(HAC, 2) # Assigner les groupes plot(X, xlab=&quot;ASAT&quot;, ylab=&quot;ALAT&quot;, pch=16, col=groupe) L’arbre est construit du bas vers le haut. On remarque bien que 2 groupes se distinguent. 6.4 Définir le nombre de classes optimales ? Dans certains cas, vous ne serez pas quel nombre de classes choisir. Nous allons définir deux approches possible vous permettant de prendre une décision et définir \\(n_c\\) (nombre de classes). Dans cet exemple, j’ai simulé 300 individus en prenant le soin de simuler un jeu de données où les variables permettent de définir un nombre de trois groupes. L’objectif de cette partie de vérifier quel algorithme est le plus performant pour retrouver le nombre de groupe réel. Je coloris les groupes que nous devons normalement retrouver. layout(matrix(c(1,2), nrow=1)) plot(X[,c(1,2)], xlab =&quot;x&quot;, ylab=&quot;y&quot;, pch=16) plot(X[,c(1,2)], xlab =&quot;x&quot;, ylab=&quot;y&quot;, col = rep(c(&quot;firebrick&quot;, &quot;cornflowerblue&quot;, &quot;purple&quot;), each=100), pch=16) Le problème que nous présentons ici est plus complexe que ceux utilisé précédement. 6.4.1 Première approche : Variance totale et règle du coude Cette première approche est une apporche empirique basée sur votre observation. Par exemple, on fait le kmeans pour différents nombres de classes et on choisit le meilleur compromis entre la variance intra-classe et le nombre de classes. On appelle inertie d’un nuage de points la moyenne des carrés des distances des \\(n\\) points au centre de gravité, soit : \\[ I_G (N) = \\frac{1}{n} \\sum{d(G, x_i)^2}\\] Par exemple pour 2 classes, la variance intra-classe se calcule de cette manière : var_intra = NULL K = 2 res.class &lt;- kmeans(X, centers = K) plot(X, col=res.class$cluster, xlab=&quot;x&quot;,ylab=&quot;y&quot; ) points(res.class$centers,pch=16, col=1:10) for(k in 1:K){ var_intra &lt;- c(var_intra, mean(as.matrix(dist(rbind(res.class$centers[k,], X[which(res.class$cluster==k),])))[-1,1])) } legend(&quot;topright&quot;,paste(&quot;variance intra-classe&quot;, round(mean(var_intra), 3)), bty=&quot;n&quot;) On calcule ensuite la variance intra-classe pour chaque nombre de clusters estimés (sur l’axe des ordonnées sur le graphique). K = 10 variance_nc = NULL for(k in 1:K){ res.class &lt;- kmeans(X, centers = k) var_intra = NULL for(j in 1:k){ var_intra &lt;- c(var_intra, mean(as.matrix(dist(rbind(res.class$centers[j,], X[which(res.class$cluster==j),])))[-1,1])) } variance_nc &lt;- c(variance_nc, mean(var_intra)) } plot(1:10, variance_nc, type=&#39;l&#39;, xlab = &quot;nb de classes&quot;, ylab=&quot;Inertie&quot;) Quelle serait le nombre de classes adapté pour notre population ? Attention, votre résultat dépend de l’initialisation des centroïdes : comme on les place aléatoirement, le résultat donné peut être différent quand on relance l’algorithme. Il est donc conseillé de relancer plusieurs fois l’algorithme pour sélectionner la partition dont l’inertie intra-classe sera la plus petite. layout(matrix(c(1,2, 3), nrow=1)) plot(X[,c(1,2)], xlab =&quot;x&quot;, ylab=&quot;y&quot;, pch=16, main=&quot;Population totale&quot;) plot(X[,c(1,2)], xlab =&quot;x&quot;, ylab=&quot;y&quot;, col = rep(c(&quot;firebrick&quot;, &quot;cornflowerblue&quot;, &quot;purple&quot;), each=100), pch=16, main=&quot;Groupes reels&quot;) plot(X[,c(1,2)], xlab =&quot;x&quot;, ylab=&quot;y&quot;, col = kmeans(X, centers = 3)$cluster, pch=16, main=&quot;Groupes estimés par le kmeans&quot;) Refaire la même chose avec la seconde méthode vue dans ce chapitre ? Quelle est la meilleure méthode pour ces données simulées ? 6.4.2 Seconde approche : coefficient de Silhouette. Coefficient de Silhouette : Évalue le degré de compacité et de séparation des grappes. En utilisant le coefficient de Silhouette, nous pouvons choisir une valeur optimale pour le nombre de groupes. Pour chaque point, le coefficient de silhouette est calculé en soustrayant la distance moyenne avec les points de son propre groupe (cohésion) de la distance moyenne avec les points des groupes voisins (séparation). Si cette différence est négative, cela signifie que le point est plus proche des points du groupe voisin que de ceux de son propre groupe, indiquant un mauvais classement. En revanche, si la différence est positive, le point est mieux lié à son propre groupe qu’aux groupes voisins, signalant un bon classement. Le coefficient de silhouette global est la moyenne des coefficients de silhouette de tous les points. Le coefficient de Silhouette est compris entre -1 et 1 ; plus il est grand, mieux c’est. L’ensemble des points appartenant à un groupe \\({\\textstyle k}\\) est alors donné par \\({\\textstyle I_{k}=\\{i\\in [\\![1,N]\\!]/\\ C(i)=k\\}}\\). Le coefficient (ou score) de silhouette se définit d’abord sur un point \\({\\textstyle i}\\) dont le groupe est \\({\\textstyle k=C(i)}\\). Il se base sur la distance moyenne du point à son groupe : \\({\\textstyle a(i)={\\frac {1}{\\vert I_{k}\\vert -1}}\\sum _{j\\in I_{k},j\\neq i}d(x^{i},x^{j})}\\) et la distance moyenne du point à son groupe voisin \\({\\textstyle b(i)=\\min _{k&#39;\\neq k}{\\frac {1}{\\vert I_{k&#39;}\\vert }}\\sum _{i&#39;\\in I_{k&#39;}}d(x^{i},x^{i&#39;})}\\). Le coefficient de silhouette du point \\({\\textstyle i}\\) s’écrit alors : \\[{\\displaystyle s_{sil}(i)={\\frac {b(i)-a(i)}{\\max(a(i),b(i))}}}\\] 6.5 Travaux pratiques Durée : 1h30 Réinitialiser votre environnement. rm(list=ls()) Objectifs : Création de son propre algorithme de clustering de type kmeans. Simuler les données sigma &lt;- matrix(0, ncol=2,nrow=2) diag(sigma) &lt;- c(1,0.4) X &lt;- rbind(mvtnorm::rmvnorm(30, mean=c(0,1.5), sigma), mvtnorm::rmvnorm(30, mean=c(1,-2), sigma), mvtnorm::rmvnorm(30, mean=c(0,0), sigma)) 6.5.1 Création de votre propre algorithme Vous devrez vous aider des lignes de codes écrites dans ce chapitre pour : coder la phase d’INITIALISATION coder la phase d’ASSIGNEMENT calculer les nouveaux centres de gravités en fonction de la phase précédente. concaténer les trois functions dans une seule et même function. 6.5.2 Efficacité et comparaison de l’algorithme Modifier les moyennes des données simulées pour voir comment réagit votre algorithme. Est-il tout le temps efficace ? Ajouter des individus les données excentrées X = rbind(X,c(3,5)) X = rbind(X,c(5,3)) X = rbind(X,c(5,5)) Refaire le clustering, qu’observez-vous ? Tester l’algorithme HAC avec différentes distances ? 6.5.3 Clustering Implémenter le jeu de données Morpho2.xlsx sur R que vous pouvez télécharger ICI. Utiliser la library readxl et sa fonction read.excel(). Tester l’algorithme des kmeans sur les colonnes de 2:6. Combien de groupes choisir ? A quoi correspond la colonne 1 ? Manipuler l’information avec la fonction substr() ou bien grep() pour différencier les hommes des femmes. Interpréter les groupes avec les variables sexe et IMC. Refaire la même chose avec l’algorithme HAC. Conclure. "],["analyse-en-composantes-principales.html", "Chapter 7 Analyse en Composantes Principales 7.1 Généralités 7.2 Réaliser la PCA sur R. 7.3 Interprétation 7.4 Travaux pratiques", " Chapter 7 Analyse en Composantes Principales Les objectifs de ce chapitre sont de comprendre les outils qui permettent d’explorer des données multivariées. 7.1 Généralités L’objectif est de décrire sans à priori un tableau de données constitué exclusivement de variables quantitatives. L’ACP permet de déterminer les espaces de dimension inférieure à l’espace initial sur lesquels la projection du nuage de points initial soit la moins déformée possible, autrement dit celle qui conserve le plus d’information c’est-à-dire de variabilité. Le principe de l’ACP est de trouver un axe (la première composante principale), issu d’une combinaison linéaire des variables initiales, tel que la variance du nuage autour de cet axe soit maximale. Et de réitérer ce processus dans des directions orthogonales pour déterminer les composantes principales suivantes. Du point de vue des variables, l’ACP permet de conserver au mieux la structure de corrélation entre les variables initiales. 7.1.1 Etude des variables La PCA va rechercher les ressemblances entre les variables (on parle de liaisons). Vous avez déjà vu dans le cours une statistique qui permet de lier deux variables quantitatives entre elles : le coefficient de corrélation. Deux variables qui seront très liées auront un coefficient de corrélation proche de 1. Ainsi nous allons visualiser la matrice de correlation (ou covariance) entre les variables pour savoir celles qui se rapprochent. La PCA permet de classer les variables et les individus en fonction de leur similarité ou de leur dissimilarité. 7.1.2 Normalisation des individus : pré-requis La première étape pour étudier les variables et les individus est de centrer et reduire les données, à partir de cette formule : \\[ X_{ij} = \\frac{X_{ij}-\\hat{\\mu}_j}{\\hat{S}_j} \\] où \\(\\hat{\\mu}_j\\) est la moyenne et \\(\\hat{S}_j\\) est l’écart type de la variable \\(j\\). La normalisation permet de centrer les données en 0 et de reduire l’écart type à 1. La normalisation permet de comparer des données qui ne possèdent pas la même unité de mesure. 7.2 Réaliser la PCA sur R. Nous réaliserons la PCA sur le jeu de données hepthatlon.csv Importer les données. df &lt;- read.table(&quot;data/heptathlon_2023.csv&quot;, sep=&quot;,&quot;, header=T, stringsAsFactors = T, row.names = 1) Les PC (Composantes Principales) résument l’information de la matrice \\(X_{ij}\\) individus/variables. Regarder le tableau. Que remarque t’on ? Supprimer les deux athlètes qui n’ont pas fini l’hepthatlon. na_athlete = which(is.na(df$Points)) # On sélectionne les athlètes ne possèdant pas de points. df = df[-na_athlete,] # On supprime les athlètes respectant la condition évoquée sur la dernière ligne. La PCA se réalise uniquement sur des variables quantitatives uniques qui caractérisent les individus. Ici, sélectionner uniquement les variables de performances (On supprime les variables Pays et Points. Pays est une variable qualitative et Points est une variable construite par rapport à l’ensemble des variables de performance.). var.perf = 3:9 # Les variables de performances vont de la colonne 3 à 9. La variable 800m est une variable de caractère. Il faut d’abord la transformer en variable quantitatif. print(paste(&quot;La première valeur de la variable 800m est conditionnée comme suit :&quot;,df$X800m[1])) ## [1] &quot;La première valeur de la variable 800m est conditionnée comme suit : 2m13s62&quot; df$X800m = as.numeric(paste0(as.numeric(substr(df$X800m, 1, 1))*60+as.numeric(substr(df$X800m, 3, 4)),&quot;.&quot;,substr(df$X800m, 6, 7))) # On transforme les données de 800m. print(paste(&quot;Après notre transformation, la première valeur de la variable 800m est conditionnée comme suit :&quot;,df$X800m[1])) ## [1] &quot;Après notre transformation, la première valeur de la variable 800m est conditionnée comme suit : 133.62&quot; Effectuer la normalisation via la fonction scale(). Expliquer le tableau normalisé. df_quanti = data.frame(scale(df[,var.perf])) # On scale dans un data.frame les variables sélectionnées. Importer la library FactoMineR et réaliser la PCA. library(FactoMineR) res.pca = PCA(df_quanti, graph = F) plot(res.pca, choix = &quot;var&quot;) plot(res.pca, choix = &quot;ind&quot;) 7.3 Interprétation Cette partie interprétation doit être bien compris. Une PCA permet de visualiser l’inertie du nuage de points en fonction des liaisons entre ces variables. Les variables fortement liées vont orienter les individus selon un même axe. Autrement dit, la PCA permet d’explorer l’hétérogénéité du jeu de données et de réduire les variables en facteurs (Dimensions ou Composantes Principales ou Principal Component). 7.3.1 Graphique des individus Le graphique des individus est le premier graphique qui apparait lorsque l’on réalise une PCA. Il place les individus dans un nouvel espace de représentation en 2 dimensions, où sont représentés les deux premières Composantes (PC ou CP en français). Vous pouvez aussi représenter les composantes 3 et 4 et ainsi de suite. Il y a autant de composantes que de variables. La dimension 1 résume environ 29% du jeu de données alors que le dimension 2 résume environ 21% du jeu de données. Vous pouvez ainsi visualiser l’explication de chaque nouveau facteur/dimension (PC) du jeu de donnée en explorant les sorties du modèle comme suit : res.pca$eig ## eigenvalue percentage of variance cumulative percentage of variance ## comp 1 2.0171883 28.816976 28.81698 ## comp 2 1.5191683 21.702404 50.51938 ## comp 3 1.0968695 15.669564 66.18894 ## comp 4 0.7585941 10.837059 77.02600 ## comp 5 0.6814221 9.734601 86.76060 ## comp 6 0.5313766 7.591095 94.35170 ## comp 7 0.3953810 5.648301 100.00000 Si nous restons focalisés sur le graph des individus, nous pouvez dire par exemple que Conte possède des coordonnées positives sur l’axe 2 et négative sur l’axe 1. Elle s’oppose à Posch ou Kalin. Conte possède des résultats très dissimilaires à Posch ou Kalin, alors que Posch possède des résultats proches de Kalin. 7.3.2 Graphique des variables (cercle de correlation des variables) 1) Lien entre les variables La première chose que vous pouvez déduire des variables, ce sont leur lien, donc potentiellement leur correlation. Deux flêches opposées signifie que la correlation est proche de -1. Deux flêches qui vont dans la même direction signifie que les variables ont un coefficient de correlation proche de 1. Dans les deux cas, nous pouvons dire que le lien est fort. La PCA résume alors l’information de ces groupes de variables en un seul axe. Lorsque deux variables possèdent un angle à 90° avec une autre variable alors leur lien (coef. de correlation) est proche de 0. Combien de groupes de variables observons-nous ? (b.1) Expliquer le lien entre les variables 200m et Longueur. Les deux variables sont t’elle résumées selon un seul axe ? (b.2) Expliquer le lien entre les variables 100mH et Poids Les deux variables sont t’elle résumées selon un seul axe ? 2) Interprétation des axes Il faut interpréter les axes en fonction du cercle des corrélations. 3) Lecture des individus Nous allons lire maintenant le graphique des individus en fonction du graphique des variables. L’interprétation des individus se réalise grâce au cerle de correlation des variables. Que dire des trois hepthatlètes Conte, Posch et Kalin ? 7.3.3 Compréhension des axes Vous pouvez aussi interpréter les axes en ajoutant des variables qualitatives ou quantitatives supplémentaires directement sur la fonction R de la PCA. Ces variables supplémentaires ne rentrent pas dans la construction des axes (PC). points = df$Points # On attribut les points des athlètes dans une nouvelle variable. df_quanti$points = points # On créé une nouvelle colonne à laquelle on attribut la variable points. res.pca = PCA(df_quanti, quanti.sup = 8) # La variable supplémentaire est la variable Points qui se situe à la 8ième colonne du tableau. Que peut-on dire de l’Axe 1 ? 7.4 Travaux pratiques rm(list=ls()) Durée : 1h30 Ce TP vise à initier aux différentes phases de l’analyse en composantes principales : préparation des données, mise en oeuvre de l’ACP, réalisation des graphiques, interprétation. 7.4.1 Préparation des données Implémenter le jeu de données Morpho2.xlsx sur R que vous pouvez télécharger ICI. Utiliser la library readxl et sa fonction read.excel(). Utiliser la fonction head() pour voir les premières lignes de la table.Quelles sont les variables de la table de données? Combien d’observations/individus ont été inclus ? La table contient des données manquantes. On peut le vérifier à l’aide de la commande is.na(). Quelles variables présentent des données manquantes ? A quoi correspond la colonne 1 ? 7.4.2 Analyse en composantes principales Créer un tableau nommé X qui contient uniquement les données morphologiques. Centrer et réduire ce tableau à l’aide de la fonction scale(). Réaliser l’analyse en composantes principales avec l’aide de la fonction PCA() de la librarie FactoMineR. 7.4.3 Interprétation des axes Tracer un diagramme en barres pour montrer la décroissance des valeurs propres. Combien d’inertie est expliquée par les 2 premiers axes ? Combien d’axes doit-on conserver pour expliquer au moins 70% d’inertie ? Combien d’axes proposez vous de conserver pour la suite de l’analyse ? Pourquoi ? Tracer, pour les axes factoriels conservés, des diagrammes en barres pour illustrer leur composition (en fonction des variables d’origine). Quelles variables expliquent le mieux le premier axe factoriel ? Le second ? Réaliser l’analyse en composantes principales avec l’aide de la fonction PCA(), en ajoutant la variable sexe en variable qualitative supplémentaire, et la variable IMC en variable quantitative supplémentaire. 7.4.4 Interprétation des individus D’après l’interprétation des axes. Que dire des individus avec des coordonnées positives sur l’axe 1 et négative sur l’axe 2 ? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
